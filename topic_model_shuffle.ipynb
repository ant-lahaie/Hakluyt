{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6687d104-8af5-4d36-9916-1eed74f0e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "AMER_root = r'text-data/CC_ML_FR_trimmed_morphad_lem_AMER/'\n",
    "hakluyt_AMER = PlaintextCorpusReader(AMER_root, '.*')\n",
    "eliz_stopwords = [\"i\",  \"me\",  \"my\",  \"myself\",  \"we\",  \"our\",  \"ours\",  \"ourselves\",  \"you\",  \"your\",  \"yours\",  \"yourself\",  \"yourselves\",  \"he\",  \"him\",  \"his\",  \"himself\",  \"she\",  \"her\",  \"hers\",  \"herself\",  \"it\",  \"its\",  \"itself\",  \"they\",  \"them\",  \"their\",  \"theirs\",  \"themselves\",  \"what\",  \"which\",  \"who\",  \"whom\",  \"this\",  \"that\",  \"these\",  \"those\",  \"am\",  \"is\",  \"are\",  \"was\",  \"were\",  \"be\",  \"been\",  \"being\",  \"have\",  \"has\",  \"had\",  \"having\",  \"do\",  \"does\",  \"did\",  \"doing\",  \"a\",  \"an\",  \"the\",  \"and\",  \"but\",  \"if\",  \"or\",  \"because\",  \"as\",  \"until\",  \"while\",  \"of\",  \"at\",  \"by\",  \"for\",  \"with\",  \"about\",  \"against\",  \"between\",  \"into\",  \"through\",  \"during\",  \"before\",  \"after\",  \"above\",  \"below\",  \"to\",  \"from\",  \"up\",  \"down\",  \"in\",  \"out\",  \"o\",  \"on\",  \"off\",  \"over\",  \"under\",  \"again\",  \"further\",  \"then\",  \"once\",  \"here\",  \"there\",  \"when\",  \"where\",  \"why\",  \"how\",  \"all\",  \"any\",  \"both\",  \"each\",  \"few\",  \"more\",  \"most\",  \"other\",  \"some\",  \"such\",  \"no\",  \"nor\",  \"not\",  \"only\",  \"own\",  \"same\",  \"so\",  \"than\",  \"too\",  \"very\",  \"can\",  \"will\",  \"just\",  \"should\",  \"now\",  \"art\", \"doth\", \"dost\", \"'ere\", \"hast\", \"hath\", \"hence\", \"hither\", \"nigh\", \"oft\", \"should'st\", \"thither\", \"thee\", \"thou\", \"thine\", \"thy\", \"'tis\", \"'twas\", \"wast\", \"whence\", \"wherefore\", \"whereto\", \"withal\", \"would'st\", \"ye\", \"yon\", \"yonder\"]\n",
    "hk_stopwords = ['unto','u','upon','de','also','wee','may','would','shall','hee','like','doe','could','much','every','againe','bee','might','without','well','within','yet','bene','ad','another','whereof','thereof','onely','next','himselfe','thus','untill','therefore','cum','selfe','non','ut', 'whole','little','full','neither','among','last','c','never','la','qui','ii','according','whose','either','per','along','item','al','likewise','mee','whereupon','till','able','self','el','que','mine','quae','sunt','et','although','litle','si','notwithstanding','besides','etiam','e','even','vel','alwayes', 'ever','rather','whether','still','otherwise','amongst', 'somewhat','ex','aforesaid','though','whatsoever','quam', 'ten','whereby','foorth','no', 'n','los','almost','howbeit','j', 'greatly','ac','yce', 'pro','en','ab','greatest','whereas','hoc','w','beene','doeth','eorum','con','withall','hereafter','moreover','nec','noone','omnes','del','enim','often']\n",
    "latin_stopwords = ['ab', 'ac', 'ad', 'adhuc', 'aliqui', 'aliquis', 'an', 'ante', 'apud', 'at', 'atque', 'aut', 'autem', 'cum', 'cur', 'de', 'deinde', 'dum', 'ego', 'enim', 'ergo', 'es', 'est', 'et', 'etiam', 'etsi', 'ex', 'fio', 'haud', 'hic', 'iam', 'idem', 'igitur', 'ille', 'in', 'infra', 'inter', 'interim', 'ipse', 'is', 'ita', 'magis', 'modo', 'mox', 'nam', 'ne', 'nec', 'necque', 'neque', 'nisi', 'non', 'nos', 'o', 'ob', 'per', 'possum', 'post', 'pro', 'quae', 'quam', 'quare', 'qui', 'quia', 'quicumque', 'quidem', 'quilibet', 'quis', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quo', 'quoniam', 'sed', 'si', 'sic', 'sive', 'sub', 'sui', 'sum', 'super', 'suus', 'tam', 'tamen', 'trans', 'tu', 'tum', 'ubi', 'uel', 'uero']\n",
    "pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'you', 'they', 'me', 'you', 'him', 'her', 'it', 'you', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'your', 'their', 'mine', 'yours', 'his', 'hers', 'its', 'ours', 'yours', 'theirs', 'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'yourselves', 'themselves']\n",
    "modals = ['shall','shal','shalt','should', 'can', \"can't\", 'cannot', 'could', 'will','wil', 'would', 'may', 'must', 'might', 'ought', 'need', 'have', 'has']\n",
    "directives = ['without', 'within', 'there', 'thence','away','est','towards','toward','farre','betweene','wherein','therein']\n",
    "numbers = ['least','lesse','none','halfe','one','two','second', 'three','iii','third','foure','four','five','sixe','seven','eight','nine','twelve','hundred','thousand','many','large','greater']\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "punctuation = [i for i in string.punctuation]\n",
    "stop_words = list(set(stopwords.words('english') + latin_stopwords + eliz_stopwords + hk_stopwords + ['']) - set(pronouns + modals + directives))\n",
    "stop_words_max = stop_words + pronouns + modals + directives + ['great', 'make', 'good', 'part', 'certain', 'thing', 'wherewith', 'afterward', 'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7778324e-f6b4-415c-b0ad-ae34595d265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "# import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd62c57-8153-4ab5-a352-ac68cfd4301f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"Nicolo\" + 0.012*\"Frisland\" + 0.012*\"Zichmni\" + 0.009*\"Estotiland\" + 0.007*\"Zeno\" + 0.006*\"Norway\" + 0.006*\"1380\" + 0.004*\"Drogio\" + 0.004*\"Engroneland\" + 0.004*\"fisherman\"'),\n",
       " (1,\n",
       "  '0.035*\"heir\" + 0.030*\"say\" + 0.020*\"assign\" + 0.020*\"time\" + 0.012*\"grant\" + 0.011*\"successor\" + 0.010*\"person\" + 0.009*\"Walter\" + 0.009*\"land\" + 0.009*\"sir\"'),\n",
       " (2,\n",
       "  '0.009*\"man\" + 0.008*\"country\" + 0.008*\"time\" + 0.006*\"one\" + 0.006*\"place\" + 0.006*\"many\" + 0.005*\"mean\" + 0.005*\"king\" + 0.005*\"find\" + 0.005*\"voyage\"'),\n",
       " (3,\n",
       "  '0.042*\"W\" + 0.038*\"N\" + 0.035*\"0\" + 0.025*\"E\" + 0.018*\"saint\" + 0.015*\"course\" + 0.015*\"24\" + 0.013*\"true\" + 0.013*\"etc\" + 0.012*\"noon\"'),\n",
       " (4,\n",
       "  '0.046*\"se\" + 0.041*\"por\" + 0.033*\"las\" + 0.029*\"lo\" + 0.015*\"mass\" + 0.013*\"un\" + 0.012*\"le\" + 0.011*\"alii\" + 0.011*\"pilot\" + 0.010*\"gente\"'),\n",
       " (5,\n",
       "  '0.033*\"sun\" + 0.021*\"heat\" + 0.016*\"Paracoussy\" + 0.014*\"equinoctial\" + 0.010*\"horizon\" + 0.009*\"cold\" + 0.007*\"cause\" + 0.007*\"angle\" + 0.007*\"region\" + 0.007*\"pole\"'),\n",
       " (6,\n",
       "  '0.033*\"league\" + 0.022*\"island\" + 0.021*\"cape\" + 0.021*\"land\" + 0.017*\"go\" + 0.017*\"degree\" + 0.017*\"north\" + 0.016*\"west\" + 0.016*\"lie\" + 0.015*\"south\"'),\n",
       " (7,\n",
       "  '0.024*\"passage\" + 0.024*\"sea\" + 0.021*\"America\" + 0.016*\"north\" + 0.014*\"east\" + 0.012*\"way\" + 0.011*\"prove\" + 0.011*\"experience\" + 0.011*\"northwest\" + 0.008*\"Asia\"'),\n",
       " (8,\n",
       "  '0.034*\"say\" + 0.020*\"captain\" + 0.016*\"come\" + 0.012*\"France\" + 0.011*\"Canada\" + 0.011*\"river\" + 0.010*\"go\" + 0.010*\"lord\" + 0.009*\"see\" + 0.009*\"Donnacona\"'),\n",
       " (9,\n",
       "  '0.000*\"say\" + 0.000*\"come\" + 0.000*\"man\" + 0.000*\"land\" + 0.000*\"time\" + 0.000*\"sea\" + 0.000*\"find\" + 0.000*\"country\" + 0.000*\"ship\" + 0.000*\"one\"'),\n",
       " (10,\n",
       "  '0.014*\"Thomas\" + 0.012*\"John\" + 0.006*\"Richard\" + 0.006*\"William\" + 0.005*\"Henry\" + 0.004*\"di\" + 0.004*\"’\" + 0.004*\"Roger\" + 0.003*\"Virginia\" + 0.003*\"brother\"'),\n",
       " (11,\n",
       "  '0.002*\"Royeze\" + 0.001*\"1543\" + 0.001*\"ivory\" + 0.001*\"Ramea\" + 0.001*\"Jaques\" + 0.001*\"boves\" + 0.001*\"Marini\" + 0.001*\"leatherdresser\" + 0.001*\"dryfat\" + 0.001*\"I543\"'),\n",
       " (12,\n",
       "  '0.005*\"Tienot\" + 0.005*\"raquelle\" + 0.004*\"france\" + 0.004*\"Carpont\" + 0.003*\"roy\" + 0.003*\"Blanc\" + 0.003*\"nostre\" + 0.003*\"filbeard\" + 0.003*\"damoiselle\" + 0.003*\"dame\"'),\n",
       " (13,\n",
       "  '0.001*\"foliow\" + 0.001*\"gaineful\" + 0.001*\"Exon\" + 0.001*\"407\" + 0.001*\"Estotiland\" + 0.000*\"nay\" + 0.000*\"Davis\" + 0.000*\"Sanderson\" + 0.000*\"sunshine\" + 0.000*\"come\"'),\n",
       " (14,\n",
       "  '0.010*\"Thomas\" + 0.009*\"master\" + 0.007*\"John\" + 0.006*\"William\" + 0.005*\"Edward\" + 0.003*\"Robert\" + 0.003*\"Richard\" + 0.003*\"James\" + 0.002*\"Philip\" + 0.001*\"Randall\"'),\n",
       " (15,\n",
       "  '0.004*\"deputy\" + 0.003*\"deputatis\" + 0.003*\"Agona\" + 0.003*\"haeredibus\" + 0.002*\"seu\" + 0.002*\"omnium\" + 0.002*\"eisdem\" + 0.002*\"deputati\" + 0.002*\"1540\" + 0.002*\"Cartier\"'),\n",
       " (16,\n",
       "  '0.018*\"utina\" + 0.011*\"fault\" + 0.009*\"Pemisapan\" + 0.008*\"mangoak\" + 0.008*\"Chawanook\" + 0.006*\"savage\" + 0.005*\"Roanoak\" + 0.005*\"Calos\" + 0.005*\"Menatonon\" + 0.005*\"Potanou\"'),\n",
       " (17,\n",
       "  '0.025*\"una\" + 0.011*\"di\" + 0.007*\"Columbus\" + 0.006*\"della\" + 0.006*\"’\" + 0.006*\"cosa\" + 0.005*\"terra\" + 0.004*\"che\" + 0.004*\"dar\" + 0.004*\"king\"'),\n",
       " (18,\n",
       "  '0.019*\"ship\" + 0.017*\"come\" + 0.016*\"man\" + 0.010*\"take\" + 0.010*\"one\" + 0.010*\"go\" + 0.009*\"captain\" + 0.009*\"general\" + 0.008*\"find\" + 0.008*\"two\"'),\n",
       " (19,\n",
       "  '0.014*\"come\" + 0.012*\"river\" + 0.011*\"man\" + 0.010*\"go\" + 0.010*\"say\" + 0.010*\"call\" + 0.009*\"ship\" + 0.009*\"one\" + 0.009*\"Indian\" + 0.008*\"take\"')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing first model\n",
    "H_AM_tokenized = [hakluyt_AMER.words(file) for file in hakluyt_AMER.fileids()]\n",
    "sw = stop_words_max + punctuation\n",
    "H_AM_tok_sw = []\n",
    "for doc in H_AM_tokenized:\n",
    "    H_AM_tok_sw.append([w for w in doc if not w in sw])\n",
    "    \n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(H_AM_tok_sw)\n",
    "# Term Document Frequency\n",
    "TDF = [id2word.doc2bow(text) for text in H_AM_tok_sw]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=TDF,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0c821b-c814-4479-98d0-0dbdcadd16ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['west', 'govern', 'place', 'Madoc', 'history', 'wales', 'Madoc', 'leave', 'Powels', 'come', 'see', 'son', 'people', 'T33', 'contrary', 'bear', 'wale', '1170', 'manifest', 'woman', 'country', 'people', '..', 'Humf', 'year', 'whjch', 'one', 'distance', 'honour', 'sail', 'cross', 'land', 'offer', 'country', 'get', 'go', '134', 'adventure', 'Madoc', 'live', 'take', 'enjoy', 'common', 'western', 'augment', 'ship', '•', 'Spaniard', 'arrive', 'r']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"Nicolo\" + 0.012*\"Frisland\" + 0.012*\"Zichmni\" + 0.009*\"Estotiland\" + 0.007*\"Zeno\" + 0.006*\"Norway\" + 0.006*\"1380\" + 0.004*\"Drogio\" + 0.004*\"Engroneland\" + 0.004*\"fisherman\"'),\n",
       " (1,\n",
       "  '0.035*\"heir\" + 0.030*\"say\" + 0.020*\"assign\" + 0.020*\"time\" + 0.012*\"grant\" + 0.011*\"successor\" + 0.010*\"person\" + 0.009*\"Walter\" + 0.009*\"land\" + 0.009*\"sir\"'),\n",
       " (2,\n",
       "  '0.009*\"man\" + 0.008*\"country\" + 0.008*\"time\" + 0.006*\"one\" + 0.006*\"place\" + 0.006*\"many\" + 0.005*\"mean\" + 0.005*\"king\" + 0.005*\"find\" + 0.005*\"voyage\"'),\n",
       " (3,\n",
       "  '0.042*\"W\" + 0.038*\"N\" + 0.035*\"0\" + 0.025*\"E\" + 0.018*\"saint\" + 0.015*\"course\" + 0.015*\"24\" + 0.013*\"true\" + 0.013*\"etc\" + 0.012*\"noon\"'),\n",
       " (4,\n",
       "  '0.046*\"se\" + 0.041*\"por\" + 0.033*\"las\" + 0.029*\"lo\" + 0.015*\"mass\" + 0.013*\"un\" + 0.012*\"le\" + 0.011*\"alii\" + 0.011*\"pilot\" + 0.010*\"gente\"'),\n",
       " (5,\n",
       "  '0.033*\"sun\" + 0.021*\"heat\" + 0.016*\"Paracoussy\" + 0.014*\"equinoctial\" + 0.010*\"horizon\" + 0.009*\"cold\" + 0.007*\"cause\" + 0.007*\"angle\" + 0.007*\"region\" + 0.007*\"pole\"'),\n",
       " (6,\n",
       "  '0.033*\"league\" + 0.022*\"island\" + 0.021*\"cape\" + 0.021*\"land\" + 0.017*\"go\" + 0.017*\"degree\" + 0.017*\"north\" + 0.016*\"west\" + 0.016*\"lie\" + 0.015*\"south\"'),\n",
       " (7,\n",
       "  '0.024*\"passage\" + 0.024*\"sea\" + 0.021*\"America\" + 0.016*\"north\" + 0.014*\"east\" + 0.012*\"way\" + 0.011*\"prove\" + 0.011*\"experience\" + 0.011*\"northwest\" + 0.008*\"Asia\"'),\n",
       " (8,\n",
       "  '0.034*\"say\" + 0.020*\"captain\" + 0.016*\"come\" + 0.012*\"France\" + 0.011*\"Canada\" + 0.011*\"river\" + 0.010*\"go\" + 0.010*\"lord\" + 0.009*\"see\" + 0.009*\"Donnacona\"'),\n",
       " (9,\n",
       "  '0.000*\"say\" + 0.000*\"come\" + 0.000*\"man\" + 0.000*\"land\" + 0.000*\"time\" + 0.000*\"sea\" + 0.000*\"find\" + 0.000*\"country\" + 0.000*\"ship\" + 0.000*\"one\"'),\n",
       " (10,\n",
       "  '0.014*\"Thomas\" + 0.012*\"John\" + 0.006*\"Richard\" + 0.006*\"William\" + 0.005*\"Henry\" + 0.004*\"di\" + 0.004*\"’\" + 0.004*\"Roger\" + 0.003*\"Virginia\" + 0.003*\"brother\"'),\n",
       " (11,\n",
       "  '0.002*\"Royeze\" + 0.001*\"1543\" + 0.001*\"ivory\" + 0.001*\"Ramea\" + 0.001*\"Jaques\" + 0.001*\"boves\" + 0.001*\"Marini\" + 0.001*\"leatherdresser\" + 0.001*\"dryfat\" + 0.001*\"I543\"'),\n",
       " (12,\n",
       "  '0.005*\"Tienot\" + 0.005*\"raquelle\" + 0.004*\"france\" + 0.004*\"Carpont\" + 0.003*\"roy\" + 0.003*\"Blanc\" + 0.003*\"nostre\" + 0.003*\"filbeard\" + 0.003*\"damoiselle\" + 0.003*\"dame\"'),\n",
       " (13,\n",
       "  '0.001*\"foliow\" + 0.001*\"gaineful\" + 0.001*\"Exon\" + 0.001*\"407\" + 0.001*\"Estotiland\" + 0.000*\"nay\" + 0.000*\"Davis\" + 0.000*\"Sanderson\" + 0.000*\"sunshine\" + 0.000*\"come\"'),\n",
       " (14,\n",
       "  '0.010*\"Thomas\" + 0.009*\"master\" + 0.007*\"John\" + 0.006*\"William\" + 0.005*\"Edward\" + 0.003*\"Robert\" + 0.003*\"Richard\" + 0.003*\"James\" + 0.002*\"Philip\" + 0.001*\"Randall\"'),\n",
       " (15,\n",
       "  '0.004*\"deputy\" + 0.003*\"deputatis\" + 0.003*\"Agona\" + 0.003*\"haeredibus\" + 0.002*\"seu\" + 0.002*\"omnium\" + 0.002*\"eisdem\" + 0.002*\"deputati\" + 0.002*\"1540\" + 0.002*\"Cartier\"'),\n",
       " (16,\n",
       "  '0.018*\"utina\" + 0.011*\"fault\" + 0.009*\"Pemisapan\" + 0.008*\"mangoak\" + 0.008*\"Chawanook\" + 0.006*\"savage\" + 0.005*\"Roanoak\" + 0.005*\"Calos\" + 0.005*\"Menatonon\" + 0.005*\"Potanou\"'),\n",
       " (17,\n",
       "  '0.025*\"una\" + 0.011*\"di\" + 0.007*\"Columbus\" + 0.006*\"della\" + 0.006*\"’\" + 0.006*\"cosa\" + 0.005*\"terra\" + 0.004*\"che\" + 0.004*\"dar\" + 0.004*\"king\"'),\n",
       " (18,\n",
       "  '0.019*\"ship\" + 0.017*\"come\" + 0.016*\"man\" + 0.010*\"take\" + 0.010*\"one\" + 0.010*\"go\" + 0.009*\"captain\" + 0.009*\"general\" + 0.008*\"find\" + 0.008*\"two\"'),\n",
       " (19,\n",
       "  '0.014*\"come\" + 0.012*\"river\" + 0.011*\"man\" + 0.010*\"go\" + 0.010*\"say\" + 0.010*\"call\" + 0.009*\"ship\" + 0.009*\"one\" + 0.009*\"Indian\" + 0.008*\"take\"')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomizing text and modeling again\n",
    "import random\n",
    "\n",
    "for doc in H_AM_tok_sw:\n",
    "    random.shuffle(doc)\n",
    "\n",
    "print(H_AM_tok_sw[0][:50])\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(H_AM_tok_sw)\n",
    "# Term Document Frequency\n",
    "TDF = [id2word.doc2bow(text) for text in H_AM_tok_sw]\n",
    "lda_model2 = gensim.models.ldamodel.LdaModel(corpus=TDF,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "lda_model2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ecda24-ae94-4a56-8965-eb162ba7bb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
