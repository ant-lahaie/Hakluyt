{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d608ecc-fc80-4655-b5fb-aab562568f07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Series of ledger operations:\n",
    "\n",
    "#### 0: create basic ledger from filenames\n",
    "\n",
    "#### 1: Encode Am tags from ledger into filenames; only include expl and/or enc tagged\n",
    "\n",
    "- name format:\n",
    "    - vol_chap-date-nat-enc-expl-title-pagerange\n",
    "    - enc_full enc_part enc_min enc_none\n",
    "    - expl_y expl_n\n",
    "\n",
    "underscores within features, dashes between features\n",
    "\n",
    "- for entry in ledger:\n",
    "    - if enc OR expl:\n",
    "        - locate file by vol-chap\n",
    "        - save file in new dir with new name\n",
    "\n",
    "#### 2: prepare ledger for re-integration with rest of corpus\n",
    "#### 3: update merged ledger with EN-only wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50627f58-55d0-478e-90a6-a2107978d481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "punctuation = [i for i in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e149282a-39d0-4dc3-ae23-0b3fb6f5483f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def HK_date(fileid):\n",
    "    #extracts date component from Hakluyt text file name, returns as integer\n",
    "    return(int(fileid[11:15]))\n",
    "def HK_geog(fileid):\n",
    "    #extracts geography component from Hakluyt text file name, returns as string\n",
    "    return(fileid[6:10])\n",
    "def HK_title(fileid):\n",
    "    #extracts geography component from Hakluyt text file name, returns as string\n",
    "    return(fileid[16:-15])\n",
    "def HK_vol(fileid):\n",
    "    #extracts volume component from Hakluyt text file name, returns as integer\n",
    "    return(int(fileid[:2]))\n",
    "def HK_chap(fileid):\n",
    "    #extracts chapter component from Hakluyt text file name, returns as integer\n",
    "    return(int(fileid[3:5]))\n",
    "def HK_pages(fileid):\n",
    "    #extracts page range component from Hakluyt text file name, returns as tuple of integers\n",
    "    pages_char = fileid[-11:-4]\n",
    "    pages_char_split = pages_char.split('-')\n",
    "    return(int(pages_char_split[0]), int(pages_char_split[1]))\n",
    "def HK_page_length(fileid):\n",
    "    #extracts page length component from Hakluyt text file name, returns as integer\n",
    "    first_page, last_page = HK_pages(fileid)\n",
    "    return(last_page-first_page+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b3a3bf-ba5d-4240-9e44-0edd821271ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('text-data/ledger.csv', 'w', newline='') as ledgerfile: \n",
    "    ledger = csv.writer(ledgerfile) \n",
    "    ledger.writerow(['vol', 'chap', 'geog', 'date', 'title', 'pages'])\n",
    "    filelist = os.scandir('text-data/CambridgeCore MacLehose pdfminer extract')\n",
    "    for file in filelist:\n",
    "        ledger.writerow([HK_vol(file.name), HK_chap(file.name), HK_geog(file.name), HK_date(file.name), HK_title(file.name), str(HK_pages(file.name)[0]).zfill(3)+'-'+str(HK_pages(file.name)[1]).zfill(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "089c6e7a-985a-45cd-8867-d55b86ac8c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vol(name):\n",
    "    return(name[:2])\n",
    "def get_chap(name):\n",
    "    name = name[3:]\n",
    "    return(name[:name.find('_')])\n",
    "def wordcount(text):\n",
    "    '''splits texts on whitespace and counts the segments, ignoring punctuation'''\n",
    "    tokens = text.split()\n",
    "    words = [w for w in tokens if w not in punctuation]\n",
    "    return(len(words))\n",
    "def wordcount_f(vol, chap, path):\n",
    "    '''locates file according to vol/chap in path, splits texts on whitespace and counts the segments, ignoring punctuation'''\n",
    "    filelist = os.scandir(path)\n",
    "    for entry in filelist:\n",
    "        if int(entry.name[:2]) == int(vol):\n",
    "            #for full chapter files:\n",
    "            if '.' not in chap:\n",
    "                if int(entry.name[3:5]) == int(chap):\n",
    "                    with open(entry.path, mode = 'r', encoding=\"utf8\") as f:\n",
    "                        text = f.read()\n",
    "                        return(wordcount(text))\n",
    "            else:\n",
    "                #for American sectioned files:\n",
    "                if entry.name[entry.name.find('_')+1:entry.name.find('AMER')-1] == chap:\n",
    "                    with open(entry.path, mode = 'r', encoding=\"utf8\") as f:\n",
    "                        text = f.read()\n",
    "                        return(wordcount(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76e73cc2-3574-481d-ae64-2f6126b83d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# testseg = '07_17.2_AMER_1170_The_most_ancient_voyage_and_discovery_of_the_West_Indies_performed_by_Madoc_the_sonne_of_Owen_Guined_pp.133-135.txt'\n",
    "# testfull = '09_24_AMER_1572_The_memorable_voyage_of_M_John_Chilton_to_all_the_principall_parts_of_Nueva_Espanna_pp.360-377'\n",
    "# #print(get_chap(test))\n",
    "# print(testfull[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8d09544-a538-4fbb-96b3-c0f0c5a66d71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# chap = '22.4'\n",
    "# print(chap[:chap.find('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "303da1f9-194b-4541-aca4-33b7b173823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24102\n"
     ]
    }
   ],
   "source": [
    "# #spot-wordcounts for missing lines\n",
    "# missing_files = ['11_29_AMER_1579_A_discourse_of_the_West_Indies_and_the_South_sea_written_by_Lopez_Vaz_a_Portugall_conteining_divers__pp.227-290.txt']\n",
    "# missing_paths = ['text-data/CC_ML_FR_trimmed_morphad_lem_AMER/' + f for f in missing_files]\n",
    "# for path in missing_paths:\n",
    "#     with open(path, mode = 'r', encoding=\"utf8\") as f:\n",
    "#         text = f.read()\n",
    "#         print(wordcount(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58251fe9-e515-48e5-96f7-3578fe79d55d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2: create an integrated full + AM_cat ledger\n",
    "\n",
    "1. compute & record segment wordcount\n",
    "1. [manually merge enc ledger with full ledger]\n",
    "1. compute & record full chapter wordcounts\n",
    "1. compute section fractions\n",
    "1. update the following fields from sections into parent chapters: national_perspective, exploration_component, intercultural_encounter_component, minor_intercultural_encounter_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9486af4-233b-4658-b64f-dbd1c416124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #now obsolete code merging AMledger with full ledger\n",
    "\n",
    "# cat_path = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat_EN/'\n",
    "# lem_path = 'CC_ML_FR_trimmed_cleaned_EN_cl_MAlem'\n",
    "\n",
    "# with open('text-data/ledger_merged_4.csv', mode = 'r') as ledger_csv:\n",
    "#     full_ledger = csv.DictReader(ledger_csv) \n",
    "#     with open('text-data/AMledger_wordcounts.csv', mode = 'r') as AMledger_csv:\n",
    "#         AM_ledger = csv.DictReader(AMledger_csv)\n",
    "#         #writing header row\n",
    "#         with open('text-data/ledger_merged_EN.csv', mode = 'a', newline='') as ledger_new_csv:\n",
    "#             writer = csv.DictWriter(ledger_new_csv, ledger.fieldnames)\n",
    "#             writer.writer.writerow(ledger.fieldnames)\n",
    "#             #copy over all non-Am rows from full ledger + wordcounts + sect_f = 1 (includes header #2)\n",
    "#             for row in full_ledger:\n",
    "#                 if row['geog'] != 'AMER':\n",
    "#                     #except header # 2 from wordcount/sect\n",
    "#                     if row['vol'] != 'volume':\n",
    "#                         row['word'] = wordcount_f(row['vol'], row['chap'])\n",
    "#                         row['sect_frac'] = 1\n",
    "#                     writer.writerow(row)\n",
    "#             #for Am rows:\n",
    "#             #for full chapters in am ledger, just copy am rows into the new ledgers  + sect_f = 1\n",
    "#             #when catches section: find corresponding full chapter;  + wordcount + sect_f = 1\n",
    "#             # then write in all the sectioned rows while calculating fractions\n",
    "#             #starting scan on am_ledger\n",
    "#             for row in AM_ledger:\n",
    "#                 #for non-header rows:\n",
    "#                 if row['geog'] == 'AMER':\n",
    "#                     if '.' not in row['chap']:\n",
    "#                         #for full chapters in am ledger, just copy am rows into the new ledgers +  sect_f = 1\n",
    "#                         filelist = os.scandir(parent_path)\n",
    "#                         row['sect_frac'] = 1\n",
    "#                         writer.writerow(row)\n",
    "#                     else:\n",
    "#                         #sequence of sections begins:  find corresponding full chapter row, write in + wordcount + sect_f = 1\n",
    "#                                                     # then write in all the sectioned rows while calculating fractions\n",
    "#                         #print(row)\n",
    "#                         if row['chap'][-1] == '1':\n",
    "#                             #bring over row from ful ledger\n",
    "#                             vol = row['vol']\n",
    "#                             chap = row['chap'][:row['chap'].find('.')]\n",
    "#                             print(f'looking for {vol}, {chap}')\n",
    "#                             with open('text-data/ledger_merged_1.csv', mode = 'r') as ledger3_csv:\n",
    "#                                 full_ledger2 = csv.DictReader(ledger3_csv) \n",
    "#                                 for row_f in full_ledger2:\n",
    "#                                     #print(row_f)\n",
    "#                                     if row_f['vol'] != 'volume':\n",
    "#                                         if int(row_f['vol']) == int(vol) and int(row_f['chap']) == int(chap):\n",
    "#                                             wordcount_full = wordcount_f(vol, chap)\n",
    "#                                             row_f['word'] = wordcount_full\n",
    "#                                             row_f['sect_frac'] = 1\n",
    "#                                             writer.writerow(row_f)\n",
    "#                                             break\n",
    "#                         row['sect_frac'] = int(row['word']) / wordcount_full\n",
    "#                         writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19909fe9-2806-4457-a082-c9877a3c262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #checking Eric's suggestion of 17 mil words\n",
    "# t_count = w_count = 0\n",
    "# filelist = os.scandir('text-data/CC_ML_FR_trimmed_cleaned_MAspel')\n",
    "# for entry in filelist:\n",
    "#     with open(entry.path, mode = 'r', encoding=\"utf8\") as f:\n",
    "#         text = f.read()\n",
    "#         tokens = text.split()\n",
    "#         words = [w for w in tokens if w not in punctuation]\n",
    "#         t_count += len(tokens)\n",
    "#         w_count += len(words)\n",
    "# print(f'{t_count} tokens, {w_count} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23345d6-3d35-484a-9f03-0f95b7852def",
   "metadata": {},
   "source": [
    "#### 3: update merged ledger with EN-only wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b61d820-2836-4c8e-801a-2df0842ea596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #update merged ledger with EN-only wordcounts\n",
    "# cat_path = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat_EN/'\n",
    "# lem_path = 'CC_ML_FR_trimmed_cleaned_EN_cl_MAlem'\n",
    "\n",
    "# with open('text-data/ledger_merged_3.csv', mode = 'r') as ledger_csv:\n",
    "#     full_ledger = csv.DictReader(ledger_csv) \n",
    "#     with open('text-data/ledger_merged_EN.csv', mode = 'a', newline='') as ledger_new_csv:\n",
    "#         writer = csv.DictWriter(ledger_new_csv, full_ledger.fieldnames)\n",
    "#         #writing header row\n",
    "#         writer.writeheader()\n",
    "#         #writer.writer.writerow(ledger.fieldnames)\n",
    "#         #simply update non-Am rows from full ledger with new wordcounts\n",
    "#         for row in full_ledger:\n",
    "#             #add wordcount to row\n",
    "#             if row['geog'] != 'AMER':\n",
    "#                 row['word'] = wordcount_f(row['vol'], row['chap'], 'text-data/CC_ML_FR_trimmed_cleaned_EN_cl_MAlem')\n",
    "#                 #print('non-am row')\n",
    "#             else:\n",
    "#                 #for full chapters, wordcount as usual and save into a temp variable for calculating fractions \n",
    "#                 if '.' not in row['chap']:\n",
    "#                     row['word'] = wordcount_f(row['vol'], row['chap'], 'text-data/CC_ML_FR_trimmed_cleaned_EN_cl_MAlem')\n",
    "#                     parent_wordcount = row['word'] + 0.0001 #avoid div-zero\n",
    "#                     #print('am-row non-sectioned')\n",
    "#             #for Am rows:\n",
    "#             #check for sectioned text file in AMcat folder \n",
    "#             #when catches section: find corresponding full chapter;  + wordcount + sect_f = 1\n",
    "#             # then write in all the sectioned rows while calculating fractions\n",
    "#                 else:\n",
    "#                     print('am sect check started')\n",
    "#                     AM_filelist = os.scandir(cat_path)\n",
    "#                     for entry in AM_filelist:\n",
    "#                         # complicated slicing for chapter value as get_chap above is not tuned to segmented sections\n",
    "#                         if int(get_vol(entry.name)) == int(row['vol']) and entry.name[entry.name.find('_')+1:entry.name.find('AMER')-1] == row['chap']:\n",
    "#                                 print(row, 'am row sectioned chap check passed')\n",
    "#                                 row['word'] = wordcount_f(row['vol'], row['chap'], 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat_EN')\n",
    "#                                 row['sect_frac'] = row['word'] / parent_wordcount\n",
    "#             #write row as composed in either decision tree branch\n",
    "#             writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f012c090-6c9b-4316-8e6b-4585b14705a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.6\n"
     ]
    }
   ],
   "source": [
    "# entryname = '07_23.6_AMER_1497_A_note_of_Cabots_first_discoverie_from_Robert_Fabians_Chronicle_pp.154-154'\n",
    "# print(entryname[entryname.find('_')+1:entryname.find('AMER')-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d970144-a396-4c98-bc7c-ba7968d2ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-32.3\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
