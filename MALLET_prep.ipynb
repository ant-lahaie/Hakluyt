{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9362be81",
   "metadata": {},
   "source": [
    "notebook to manage and document work with MALLET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c828cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### stopwords file composition & generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171a5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a623bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "# eliz_stopwords = [\"i\",  \"me\",  \"my\",  \"myself\",  \"we\",  \"our\",  \"ours\",  \"ourselves\",  \"you\",  \"your\",  \"yours\",  \"yourself\",  \"yourselves\",  \"he\",  \"him\",  \"his\",  \"himself\",  \"she\",  \"her\",  \"hers\",  \"herself\",  \"it\",  \"its\",  \"itself\",  \"they\",  \"them\",  \"their\",  \"theirs\",  \"themselves\",  \"what\",  \"which\",  \"who\",  \"whom\",  \"this\",  \"that\",  \"these\",  \"those\",  \"am\",  \"is\",  \"are\",  \"was\",  \"were\",  \"be\",  \"been\",  \"being\",  \"have\",  \"has\",  \"had\",  \"having\",  \"do\",  \"does\",  \"did\",  \"doing\",  \"a\",  \"an\",  \"the\",  \"and\",  \"but\",  \"if\",  \"or\",  \"because\",  \"as\",  \"until\",  \"while\",  \"of\",  \"at\",  \"by\",  \"for\",  \"with\",  \"about\",  \"against\",  \"between\",  \"into\",  \"through\",  \"during\",  \"before\",  \"after\",  \"above\",  \"below\",  \"to\",  \"from\",  \"up\",  \"down\",  \"in\",  \"out\",  \"o\",  \"on\",  \"off\",  \"over\",  \"under\",  \"again\",  \"further\",  \"then\",  \"once\",  \"here\",  \"there\",  \"when\",  \"where\",  \"why\",  \"how\",  \"all\",  \"any\",  \"both\",  \"each\",  \"few\",  \"more\",  \"most\",  \"other\",  \"some\",  \"such\",  \"no\",  \"nor\",  \"not\",  \"only\",  \"own\",  \"same\",  \"so\",  \"than\",  \"too\",  \"very\",  \"can\",  \"will\",  \"just\",  \"should\",  \"now\",  \"art\", \"doth\", \"dost\", \"'ere\", \"hast\", \"hath\", \"hence\", \"hither\", \"nigh\", \"oft\", \"should'st\", \"thither\", \"thee\", \"thou\", \"thine\", \"thy\", \"'tis\", \"'twas\", \"wast\", \"whence\", \"wherefore\", \"whereto\", \"withal\", \"would'st\", \"ye\", \"yon\", \"yonder\"]\n",
    "# hk_stopwords = ['unto','u','one', 'five','upon','de','also','wee','two','may','many','would','shall','hee','like','three','doe','could','much','every','againe','bee','might','without','well','within','yet','bene','ad','foure','another','whereof','thereof','onely','next','himselfe','thus','hundred','untill','therefore','halfe','cum','selfe','non','ut', 'whole','little','sixe','full','neither','among','last','c','never','la','qui','ii','according','eight','whose','either','per','along','item','al','likewise','mee','whereupon','none','till','able','thousand','self','el','second','que','mine','quae','sunt','et','seven','iii','although','litle','si','notwithstanding','besides','etiam','lesse','e','even','vel','alwayes', 'third','ever','rather','whether','still','otherwise','large','amongst', 'greater','somewhat','ex','least','aforesaid','though','whatsoever','quam', 'ten','whereby','foorth','no', 'n','los','almost','twelve','howbeit','j', 'greatly','ac','yce', 'pro','en','ab','greatest','whereas','hoc','w','beene','doeth','eorum','con','withall','hereafter','moreover','nec','nine','noone','omnes','del','enim','often']\n",
    "# latin_stopwords = ['ab', 'ac', 'ad', 'adhuc', 'aliqui', 'aliquis', 'an', 'ante', 'apud', 'at', 'atque', 'aut', 'autem', 'cum', 'cur', 'de', 'deinde', 'dum', 'ego', 'enim', 'ergo', 'es', 'est', 'et', 'etiam', 'etsi', 'ex', 'fio', 'haud', 'hic', 'iam', 'idem', 'igitur', 'ille', 'in', 'infra', 'inter', 'interim', 'ipse', 'is', 'ita', 'magis', 'modo', 'mox', 'nam', 'ne', 'nec', 'necque', 'neque', 'nisi', 'non', 'nos', 'o', 'ob', 'per', 'possum', 'post', 'pro', 'quae', 'quam', 'quare', 'qui', 'quia', 'quicumque', 'quidem', 'quilibet', 'quis', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quo', 'quoniam', 'sed', 'si', 'sic', 'sive', 'sub', 'sui', 'sum', 'super', 'suus', 'tam', 'tamen', 'trans', 'tu', 'tum', 'ubi', 'uel', 'uero']\n",
    "# pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'you', 'they', 'me', 'you', 'him', 'her', 'it', 'you', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'your', 'their', 'mine', 'yours', 'his', 'hers', 'its', 'ours', 'yours', 'theirs', 'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'yourselves', 'themselves']\n",
    "# modals = ['shall','shal','shalt','should', 'can', \"can't\", 'cannot' 'could', 'will','wil', 'would', 'may', 'must', 'might', 'ought', 'need', 'have', 'has']\n",
    "# directives = ['without', 'within', 'there', 'thence','away','est','towards','toward','farre','betweene','wherein','therein']\n",
    "\n",
    "# #stop_words = set(nltk_stopwords + latin_stopwords + eliz_stopwords + hk_stopwords) - set(pronouns + modals + directives)\n",
    "# stop_words_df = pd.DataFrame()\n",
    "# stop_words_df['nltk'] = nltk_stopwords\n",
    "# stop_words_df['eliz'] = pd.Series(eliz_stopwords)\n",
    "# stop_words_df['hk'] = pd.Series(hk_stopwords)\n",
    "# stop_words_df['pronouns'] = pd.Series(pronouns)\n",
    "# stop_words_df['modals'] = pd.Series(modals)\n",
    "# stop_words_df['directives'] = pd.Series(directives)\n",
    "\n",
    "\n",
    "# stop_words_df.to_csv(r'text-data/stopwords.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4efa1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_df = pd.read_csv(r'text-data/stopwords.csv', encoding='utf-8')\n",
    "# stop_words_nltk_eliz_hk_pronouns = stop_words_df['nltk'].tolist()+stop_words_df['eliz'].tolist()+stop_words_df['hk'].tolist()+stop_words_df['pronouns'].tolist()+stop_words_df['latin'].tolist()\n",
    "# stop_words_nltk_eliz_hk_pronouns = [word for word in stop_words_nltk_eliz_hk if str(word) != 'nan']\n",
    "# with open(r'mallet/stop_words_nltk_eliz_hk_pronouns.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(stop_words_nltk_eliz_hk_pronouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a2dc562-5761-4077-980b-fcb24c81480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_df = pd.read_csv(r'text-data/stopwords.csv', encoding='utf-8')\n",
    "# stop_words_all = []\n",
    "# for column in stop_words_df:\n",
    "#     stop_words_all += stop_words_df[column].tolist()\n",
    "# stop_words_all += stopwords.words('spanish')\n",
    "# stop_words_all = [word for word in stop_words_all if str(word) != 'nan']\n",
    "# with open(r'mallet/stop_words_all.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(stop_words_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75148b8c-d96c-49f6-b758-9f6b3f857b5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### chunked corpus creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be92622-645f-45d8-872c-eeeefb78d5ca",
   "metadata": {},
   "source": [
    "The median American chapter length is  2275.0  words, which, at about 360 words for a crammed page, comes to  6.319444444444445  pages.\n",
    "\n",
    "Create plaintext Am corpus, iterate on files\n",
    "* chunk function: takes text, chunk size; returns list of string - chunks:\n",
    "  *  if text longer than chunk size * 1.5:   \n",
    "     *    split text into (length mod chunk size) equal parts                                             \n",
    "* write function: takes name & list of strings, writes strings into name[i] files \n",
    "\n",
    "//setting max length at 1.5 target chunk so there's enough extra to distribute around, avoiding either a 5-word file if I was to hard-truncate the excess or effectively 2 half-pages if I split the file into uniform chunks   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd9495c-4de2-4493-bcdf-5e3511df2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def chunk_text(text, c_size):\n",
    "    '''breaks text into uniform chunks around the desired chunk size\n",
    "    arguments:\n",
    "    text: text given as list of strings\n",
    "    c_size: desired chunk size\n",
    "    returns: list of lists of strings, sized around c_size'''\n",
    "    if len(text) < int(c_size*1.5):\n",
    "        return([text])\n",
    "    else:\n",
    "        chunked = (numpy.array_split(text,len(text)//c_size))\n",
    "    return([list(chunk) for chunk in chunked])\n",
    "\n",
    "def write_chunks(path, name, chunks):\n",
    "    '''writes the text chunks as files\n",
    "    arguments:\n",
    "    path: folder path to write the files into\n",
    "    name: base name for the files\n",
    "    chunks: list of lists of strings; each of the component lists will be joined with spaces and written to file under base name plus index number\n",
    "    '''\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        #should have zfilled the chunk name--currently I have page 1 followed by 11 in the filelist\n",
    "        with open(path + r'/' + name + '_' + str(i).zfill(3) +'.txt', 'w', encoding = 'utf-8') as f:\n",
    "            f.write(' '.join(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5525686d-c0e7-4bde-b580-df4a575d474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CC_ML_FR_trimmed_MAlem_AMER_chunked_pages\n",
    "# # CC_ML_FR_trimmed_MAlem_AMER_chunked_med_chap\n",
    "#CC_ML_FR_trimmed_cleaned_EN_cl_MAlem\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "# unfiltered corpuses\n",
    "tot_corpus = PlaintextCorpusReader('text-data/CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER', '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660823d8-533f-48a9-bb93-401a7b9cc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewrite as function\n",
    "import os\n",
    "os.mkdir('text-data/CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_chunked_190w')\n",
    "for fileid in tot_corpus.fileids():\n",
    "    chunks = chunk_text(tot_corpus.words(fileid), 190)\n",
    "    write_chunks('text-data/CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_chunked_190w', fileid[:-4], chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21284080-7acb-4225-8168-ed003a806a8b",
   "metadata": {},
   "source": [
    "### MALLET command line instructions creation\n",
    "#### feed print outputs to the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c8b5f9-4d02-4326-a294-85bcb6149642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set MALLET installation directory, text corpus directory and desired output directory\n",
    "\n",
    "mal_install = r'C:\\Users\\apovzner\\Documents\\mallet-2.0.8'\n",
    "mal_texts = r'C:\\Users\\apovzner\\Documents\\Hakluyt\\text-data'\n",
    "mal_out = r'C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eac6601-bbb9-4479-bd2d-9aa806cbc0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd C:\\Users\\apovzner\\Documents\\mallet-2.0.8\n"
     ]
    }
   ],
   "source": [
    "# initialize working directory at the beginning\n",
    "print('cd ' + mal_install)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5796eb6",
   "metadata": {},
   "source": [
    "**mallet corpus creation**\n",
    "adjust input directory depending on the corpus of interest and stoplist-file to match desired stopword filtering\n",
    "naming the output file through the corpus content and stop word specification parameters will help you keep track of what went into each mallet corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864c1c96-2283-45fb-8088-848f5e9faf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\\mallet import-dir --input C:\\Users\\apovzner\\Documents\\Hakluyt\\text-data\\CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER --output C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER_sw_all.mallet --keep-sequence --stoplist-file C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\stop_words_all.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = 'CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER'\n",
    "print(r'bin\\mallet import-dir --input ' + mal_texts + '\\\\' + corpus + r' --output ' + mal_out + '\\\\' + corpus + '_sw_all.mallet' + ' --keep-sequence --stoplist-file ' + mal_out + r'\\stop_words_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8461e-2546-411b-8d5c-ac0acf37544b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### n-grams corpus creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f7dcd-33c5-44fe-b18c-38f510cc7908",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "There's a bug causing an n-gram corpus (i.e., sampling sequences of n words instead of--or in addition to--every separate word) to fail; see solution below (from https://mallet-dev.cs.umass.narkive.com/QvKRARcb/help-with-mallet-in-windows):\n",
    "\n",
    "<div>\n",
    "<hr/>\n",
    "Go to bin folder, open mallet.bat file, then, find the following line <br>\n",
    "if \"%1\"==\"\" goto run <br>\n",
    "set MALLET_ARGS=%MALLET_ARGS% %1 <br>\n",
    "Replace with the following: <br>\n",
    "if \"%~1\"==\"\" goto run <br>\n",
    "set MALLET_ARGS=%MALLET_ARGS% \"%~1\" <br>\n",
    "<br>\n",
    "In the mallet command line, use double quote --gram-size \"1,2\"\n",
    "<hr/>\n",
    "</div>\n",
    "<br>\n",
    "adjust the desired n-gram sizes in the gram-sizes parameter; note that adding n-grams is highly computationally intensive. I never went above \"1,2\"--that is, counting both separate words and bigrams--and I saw no measurable improvement, so I dropped it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2a8ab03-d462-493a-8350-9855c0e10552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\\mallet import-dir --input C:\\Users\\apovzner\\Documents\\Hakluyt\\text-data\\CC_ML_FR_trimmed_morphad_lem --output C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\FR_trim_MA_stop_words_all_ngrams.mallet --keep-sequence-bigrams --stoplist-file C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\stop_words_all.txt --gram-sizes \"1,2\"\n"
     ]
    }
   ],
   "source": [
    "print(r'bin\\mallet import-dir --input ' + mal_texts + r'\\CC_ML_FR_trimmed_morphad_lem' + r' --output ' + mal_out + r'\\FR_trim_MA_stop_words_all_ngrams.mallet' + ' --keep-sequence-bigrams --stoplist-file ' + mal_out +r'\\stop_words_all.txt --gram-sizes \"1,2\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83035db9",
   "metadata": {},
   "source": [
    "#### mallet topic model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72444af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\\mallet train-topics --input C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER_sw_all.mallet --num-topics 40 --num-iterations 10000 --optimize-interval 5 --optimize-burn-in 10 --output-state C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER_sw_all_40_topics_10000_iter_5_10_optim_diag_.gz --output-doc-topics C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER_sw_all_40_topics_10000_iter_5_10_optim_diag__doctopics.txt --output-topic-keys C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER_sw_all_40_topics_10000_iter_5_10_optim_diag__topickeys.txt --diagnostics-file C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER_sw_all_40_topics_10000_iter_5_10_optim_diag__diagnostics.xml --xml-topic-phrase-report C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER_sw_all_40_topics_10000_iter_5_10_optim_diag__topicphrase.xml --num-top-words 50\n"
     ]
    }
   ],
   "source": [
    "num_topics = 40\n",
    "num_iterations = 10000\n",
    "optimize_interval = 5\n",
    "optimize_burn_in = 10\n",
    "folder_path = mal_out +'\\\\'\n",
    "corpus = 'CC_ML_FR_trimmed_cleaned_EN_cl_MAlem_AMER'\n",
    "stop_words = 'all'\n",
    "extra = '_diag_'\n",
    "output_name = corpus + '_sw_' + str(stop_words) + '_' + str(num_topics) + '_topics_' + str(num_iterations) + '_iter_' + str(optimize_interval) + '_' + str(optimize_burn_in) + '_optim' + extra\n",
    "\n",
    "print(f'bin\\mallet train-topics --input {folder_path}{corpus}_sw_{stop_words}.mallet --num-topics {num_topics} --num-iterations {num_iterations} --optimize-interval {optimize_interval} --optimize-burn-in {optimize_burn_in} --output-state {folder_path}{output_name}.gz --output-doc-topics {folder_path}{output_name}_doctopics.txt --output-topic-keys {folder_path}{output_name}_topickeys.txt --diagnostics-file {folder_path}{output_name}_diagnostics.xml --xml-topic-phrase-report {folder_path}{output_name}_topicphrase.xml --num-top-words 50')\n",
    "#print(f'bin\\mallet train-topics --input {folder_path}{corpus}_sw_{stop_words}{extra}.mallet --num-topics {num_topics} --num-iterations {num_iterations} --optimize-interval {optimize_interval} --optimize-burn-in {optimize_burn_in} --output-state {folder_path}{output_name}.gz --output-doc-topics {folder_path}{output_name}_doctopics.txt --output-topic-keys {folder_path}{output_name}_topickeys.txt --num-top-words 50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e895f-dd23-4b9e-9ae0-a745b688b359",
   "metadata": {},
   "source": [
    "##### a useful note on topic-keys numbers:\n",
    "\n",
    "The numbers you see are the parameters of a Dirichlet distribution that describes our prior expectation of the mix of topics in a document. You can think of it as having two parts: proportions and magnitude. If you rescale the numbers to add up to 1.0, the resulting proportions would tell you the model's guess at which topics occur most frequently. The actual sum of the numbers (the magnitude) tells you how confident the model is that this is the actual proportion you will see in a document. Smaller values indicate more variability.\n",
    "\n",
    "From <https://stackoverflow.com/questions/59458102/topic-modeling-with-mallet-topic-keys-output-parameter> \n",
    "\n",
    "##### for more exact proportions:\n",
    "In the XML diagnostics file, see tokens: \"This metric measures the number of word tokens currently assigned to the topic Nk=∑dNd,k. Comparing this number to the sum of the token counts for all topics will give you the proportion of the corpus assigned to the topic.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
