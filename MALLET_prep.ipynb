{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9362be81",
   "metadata": {},
   "source": [
    "notebook to manage and document work with MALLET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c828cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### stopwords file composition & generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171a5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a623bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "# eliz_stopwords = [\"i\",  \"me\",  \"my\",  \"myself\",  \"we\",  \"our\",  \"ours\",  \"ourselves\",  \"you\",  \"your\",  \"yours\",  \"yourself\",  \"yourselves\",  \"he\",  \"him\",  \"his\",  \"himself\",  \"she\",  \"her\",  \"hers\",  \"herself\",  \"it\",  \"its\",  \"itself\",  \"they\",  \"them\",  \"their\",  \"theirs\",  \"themselves\",  \"what\",  \"which\",  \"who\",  \"whom\",  \"this\",  \"that\",  \"these\",  \"those\",  \"am\",  \"is\",  \"are\",  \"was\",  \"were\",  \"be\",  \"been\",  \"being\",  \"have\",  \"has\",  \"had\",  \"having\",  \"do\",  \"does\",  \"did\",  \"doing\",  \"a\",  \"an\",  \"the\",  \"and\",  \"but\",  \"if\",  \"or\",  \"because\",  \"as\",  \"until\",  \"while\",  \"of\",  \"at\",  \"by\",  \"for\",  \"with\",  \"about\",  \"against\",  \"between\",  \"into\",  \"through\",  \"during\",  \"before\",  \"after\",  \"above\",  \"below\",  \"to\",  \"from\",  \"up\",  \"down\",  \"in\",  \"out\",  \"o\",  \"on\",  \"off\",  \"over\",  \"under\",  \"again\",  \"further\",  \"then\",  \"once\",  \"here\",  \"there\",  \"when\",  \"where\",  \"why\",  \"how\",  \"all\",  \"any\",  \"both\",  \"each\",  \"few\",  \"more\",  \"most\",  \"other\",  \"some\",  \"such\",  \"no\",  \"nor\",  \"not\",  \"only\",  \"own\",  \"same\",  \"so\",  \"than\",  \"too\",  \"very\",  \"can\",  \"will\",  \"just\",  \"should\",  \"now\",  \"art\", \"doth\", \"dost\", \"'ere\", \"hast\", \"hath\", \"hence\", \"hither\", \"nigh\", \"oft\", \"should'st\", \"thither\", \"thee\", \"thou\", \"thine\", \"thy\", \"'tis\", \"'twas\", \"wast\", \"whence\", \"wherefore\", \"whereto\", \"withal\", \"would'st\", \"ye\", \"yon\", \"yonder\"]\n",
    "# hk_stopwords = ['unto','u','one', 'five','upon','de','also','wee','two','may','many','would','shall','hee','like','three','doe','could','much','every','againe','bee','might','without','well','within','yet','bene','ad','foure','another','whereof','thereof','onely','next','himselfe','thus','hundred','untill','therefore','halfe','cum','selfe','non','ut', 'whole','little','sixe','full','neither','among','last','c','never','la','qui','ii','according','eight','whose','either','per','along','item','al','likewise','mee','whereupon','none','till','able','thousand','self','el','second','que','mine','quae','sunt','et','seven','iii','although','litle','si','notwithstanding','besides','etiam','lesse','e','even','vel','alwayes', 'third','ever','rather','whether','still','otherwise','large','amongst', 'greater','somewhat','ex','least','aforesaid','though','whatsoever','quam', 'ten','whereby','foorth','no', 'n','los','almost','twelve','howbeit','j', 'greatly','ac','yce', 'pro','en','ab','greatest','whereas','hoc','w','beene','doeth','eorum','con','withall','hereafter','moreover','nec','nine','noone','omnes','del','enim','often']\n",
    "# latin_stopwords = ['ab', 'ac', 'ad', 'adhuc', 'aliqui', 'aliquis', 'an', 'ante', 'apud', 'at', 'atque', 'aut', 'autem', 'cum', 'cur', 'de', 'deinde', 'dum', 'ego', 'enim', 'ergo', 'es', 'est', 'et', 'etiam', 'etsi', 'ex', 'fio', 'haud', 'hic', 'iam', 'idem', 'igitur', 'ille', 'in', 'infra', 'inter', 'interim', 'ipse', 'is', 'ita', 'magis', 'modo', 'mox', 'nam', 'ne', 'nec', 'necque', 'neque', 'nisi', 'non', 'nos', 'o', 'ob', 'per', 'possum', 'post', 'pro', 'quae', 'quam', 'quare', 'qui', 'quia', 'quicumque', 'quidem', 'quilibet', 'quis', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quo', 'quoniam', 'sed', 'si', 'sic', 'sive', 'sub', 'sui', 'sum', 'super', 'suus', 'tam', 'tamen', 'trans', 'tu', 'tum', 'ubi', 'uel', 'uero']\n",
    "# pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'you', 'they', 'me', 'you', 'him', 'her', 'it', 'you', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'your', 'their', 'mine', 'yours', 'his', 'hers', 'its', 'ours', 'yours', 'theirs', 'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'yourselves', 'themselves']\n",
    "# modals = ['shall','shal','shalt','should', 'can', \"can't\", 'cannot' 'could', 'will','wil', 'would', 'may', 'must', 'might', 'ought', 'need', 'have', 'has']\n",
    "# directives = ['without', 'within', 'there', 'thence','away','est','towards','toward','farre','betweene','wherein','therein']\n",
    "\n",
    "# #stop_words = set(nltk_stopwords + latin_stopwords + eliz_stopwords + hk_stopwords) - set(pronouns + modals + directives)\n",
    "# stop_words_df = pd.DataFrame()\n",
    "# stop_words_df['nltk'] = nltk_stopwords\n",
    "# stop_words_df['eliz'] = pd.Series(eliz_stopwords)\n",
    "# stop_words_df['hk'] = pd.Series(hk_stopwords)\n",
    "# stop_words_df['pronouns'] = pd.Series(pronouns)\n",
    "# stop_words_df['modals'] = pd.Series(modals)\n",
    "# stop_words_df['directives'] = pd.Series(directives)\n",
    "\n",
    "\n",
    "# stop_words_df.to_csv(r'text-data/stopwords.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4efa1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_df = pd.read_csv(r'text-data/stopwords.csv', encoding='utf-8')\n",
    "# stop_words_nltk_eliz_hk_pronouns = stop_words_df['nltk'].tolist()+stop_words_df['eliz'].tolist()+stop_words_df['hk'].tolist()+stop_words_df['pronouns'].tolist()+stop_words_df['latin'].tolist()\n",
    "# stop_words_nltk_eliz_hk_pronouns = [word for word in stop_words_nltk_eliz_hk if str(word) != 'nan']\n",
    "# with open(r'mallet/stop_words_nltk_eliz_hk_pronouns.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(stop_words_nltk_eliz_hk_pronouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc74b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_df = pd.read_csv(r'text-data/stopwords.csv', encoding='utf-8')\n",
    "# stop_words_all = []\n",
    "# for column in stop_words_df:\n",
    "#     stop_words_all += stop_words_df[column].tolist()\n",
    "# stop_words_all += stopwords.words('spanish')\n",
    "# stop_words_all = [word for word in stop_words_all if str(word) != 'nan']\n",
    "# with open(r'mallet/stop_words_all.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(stop_words_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21284080-7acb-4225-8168-ed003a806a8b",
   "metadata": {},
   "source": [
    "### MALLET command line instructions creation\n",
    "#### feed print outputs to the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30c8b5f9-4d02-4326-a294-85bcb6149642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set MALLET installation directory, text corpus directory and desired output directory\n",
    "\n",
    "mal_install = r'C:\\Users\\apovzner\\Documents\\mallet-2.0.8'\n",
    "mal_texts = r'C:\\Users\\apovzner\\Documents\\Hakluyt\\text-data'\n",
    "mal_out = r'C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eac6601-bbb9-4479-bd2d-9aa806cbc0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd C:\\Users\\apovzner\\Documents\\mallet-2.0.8\n"
     ]
    }
   ],
   "source": [
    "# initialize working directory at the beginning\n",
    "\n",
    "print('cd ' + mal_install)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5796eb6",
   "metadata": {},
   "source": [
    "**mallet corpus creation**\n",
    "adjust input directory depending on the corpus of interest and stoplist-file to match desired stopword filtering\n",
    "naming the output file through the corpus content and stop word specification parameters will help you keep track of what went into each mallet corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "864c1c96-2283-45fb-8088-848f5e9faf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\\mallet import-dir --input C:\\Users\\apovzner\\Documents\\Hakluyt\\text-data\\CC_ML_FR_trimmed_morphad_lem --output C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\FR_trim_MA_stop_words_all.mallet --keep-sequence --stoplist-file C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\stop_words_all.txt\n"
     ]
    }
   ],
   "source": [
    "print(r'bin\\mallet import-dir --input ' + mal_texts + r'\\CC_ML_FR_trimmed_morphad_lem' + r' --output ' + mal_out + r'\\FR_trim_MA_stop_words_all.mallet' + ' --keep-sequence --stoplist-file ' + mal_out +r'\\stop_words_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb1319",
   "metadata": {},
   "source": [
    "**n-grams corpus creation**\n",
    "\n",
    "There's a bug causing an n-gram corpus (i.e., sampling sequences of n words instead of--or in addition to--every separate word) to fail; see solution below (from https://mallet-dev.cs.umass.narkive.com/QvKRARcb/help-with-mallet-in-windows):\n",
    "\n",
    "<div>\n",
    "<hr/>\n",
    "Go to bin folder, open mallet.bat file, then, find the following line <br>\n",
    "if \"%1\"==\"\" goto run <br>\n",
    "set MALLET_ARGS=%MALLET_ARGS% %1 <br>\n",
    "Replace with the following: <br>\n",
    "if \"%~1\"==\"\" goto run <br>\n",
    "set MALLET_ARGS=%MALLET_ARGS% \"%~1\" <br>\n",
    "<br>\n",
    "In the mallet command line, use double quote --gram-size \"1,2\"\n",
    "<hr/>\n",
    "</div>\n",
    "<br>\n",
    "adjust the desired n-gram sizes in the gram-sizes parameter; note that adding n-grams is highly computationally intensive. I never went above \"1,2\"--that is, counting both separate words and bigrams--and I saw no measurable improvement, so I dropped it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2a8ab03-d462-493a-8350-9855c0e10552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\\mallet import-dir --input C:\\Users\\apovzner\\Documents\\Hakluyt\\text-data\\CC_ML_FR_trimmed_morphad_lem --output C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\FR_trim_MA_stop_words_all_ngrams.mallet --keep-sequence-bigrams --stoplist-file C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\stop_words_all.txt --gram-sizes \"1,2\"\n"
     ]
    }
   ],
   "source": [
    "print(r'bin\\mallet import-dir --input ' + mal_texts + r'\\CC_ML_FR_trimmed_morphad_lem' + r' --output ' + mal_out + r'\\FR_trim_MA_stop_words_all_ngrams.mallet' + ' --keep-sequence-bigrams --stoplist-file ' + mal_out +r'\\stop_words_all.txt --gram-sizes \"1,2\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83035db9",
   "metadata": {},
   "source": [
    "**mallet topic model creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72444af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\\mallet train-topics --input C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\FR_trim_MA_AMER_enc_part_full_stop_words_all.mallet --num-topics 15 --num-iterations 5000 --optimize-interval 5 --optimize-burn-in 10 --output-state C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\FR_trim_MA_AMER_enc_part_full_sw_all_15_topics_5000_iter_5_10_optim.gz --output-doc-topics C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\FR_trim_MA_AMER_enc_part_full_sw_all_15_topics_5000_iter_5_10_optim_doctopics.txt --output-topic-keys C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\FR_trim_MA_AMER_enc_part_full_sw_all_15_topics_5000_iter_5_10_optim_topickeys.txt --diagnostics-file C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\FR_trim_MA_AMER_enc_part_full_sw_all_15_topics_5000_iter_5_10_optim_diagnostics.xml --xml-topic-phrase-report C:\\Users\\apovzner\\Documents\\Hakluyt\\mallet\\FR_trim_MA_AMER_enc_part_full_sw_all_15_topics_5000_iter_5_10_optim_topicphrase.xml --num-top-words 50\n"
     ]
    }
   ],
   "source": [
    "num_topics = 15\n",
    "num_iterations = 5000\n",
    "optimize_interval = 5\n",
    "optimize_burn_in = 10\n",
    "folder_path = mal_out +'\\\\'\n",
    "corpus = 'FR_trim_MA_AMER_enc_part_full'\n",
    "stop_words = 'all'\n",
    "extra = ''\n",
    "output_name = corpus + '_sw_' + str(stop_words) + '_' + str(num_topics) + '_topics_' + str(num_iterations) + '_iter_' + str(optimize_interval) + '_' + str(optimize_burn_in) + '_optim' + extra\n",
    "\n",
    "print(f'bin\\mallet train-topics --input {folder_path}{corpus}_stop_words_{stop_words}{extra}.mallet --num-topics {num_topics} --num-iterations {num_iterations} --optimize-interval {optimize_interval} --optimize-burn-in {optimize_burn_in} --output-state {folder_path}{output_name}.gz --output-doc-topics {folder_path}{output_name}_doctopics.txt --output-topic-keys {folder_path}{output_name}_topickeys.txt --diagnostics-file {folder_path}{output_name}_diagnostics.xml --xml-topic-phrase-report {folder_path}{output_name}_topicphrase.xml --num-top-words 50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e895f-dd23-4b9e-9ae0-a745b688b359",
   "metadata": {},
   "source": [
    "##### a useful note on topic-keys numbers: #####\n",
    "\n",
    "The numbers you see are the parameters of a Dirichlet distribution that describes our prior expectation of the mix of topics in a document. You can think of it as having two parts: proportions and magnitude. If you rescale the numbers to add up to 1.0, the resulting proportions would tell you the model's guess at which topics occur most frequently. The actual sum of the numbers (the magnitude) tells you how confident the model is that this is the actual proportion you will see in a document. Smaller values indicate more variability.\n",
    "\n",
    "From <https://stackoverflow.com/questions/59458102/topic-modeling-with-mallet-topic-keys-output-parameter> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
