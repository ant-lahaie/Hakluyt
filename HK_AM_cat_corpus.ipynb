{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d608ecc-fc80-4655-b5fb-aab562568f07",
   "metadata": {},
   "source": [
    "### work on hand-tagged American intercultural encounter segments\n",
    "\n",
    "#### 1: Encode Am tags from ledger into filenames; only include expl and/or enc tagged\n",
    "\n",
    "- name format:\n",
    "    - vol_chap-date-nat-enc-expl-title-pagerange\n",
    "    - enc_full enc_part enc_min enc_none\n",
    "    - expl_y expl_n\n",
    "\n",
    "underscores within features, dashes between features\n",
    "\n",
    "- for entry in ledger:\n",
    "    - if enc OR expl:\n",
    "        - locate file by vol-chap\n",
    "        - save file in new dir with new name\n",
    "\n",
    "#### 2: prepare ledger for re-integration with rest of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fe0ee12-60ae-4b73-b8cb-56bc680f7e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in retrospect, the code below missed a bunch of files; not sure what happened\n",
    "old_path = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat/'\n",
    "new_path = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat_mod/'\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "with open('text-data/AMledger.csv', mode = 'r') as ledger_csv:\n",
    "    ledger = csv.DictReader(ledger_csv)\n",
    "    for row in ledger:\n",
    "        #print('starting row')\n",
    "        if 'v' in row['expl'] or 'v' in row['enc'] or 'v' in row['enc_min']:\n",
    "            #print('row of interest')\n",
    "            # if entry addresses travel or cultural encounter:\n",
    "            filelist = os.scandir(old_path)\n",
    "            for entry in filelist:\n",
    "                #print('scanning' , entry.name)\n",
    "                #print ( 'condition checks: ', row['vol'].zfill(2), re.search(r'_(.*?)_', entry.name).group(1), row['vol'].zfill(2) == entry.name[:2], row['chap'] == re.search(r'_(.*?)_', entry.name).group(1))\n",
    "                if row['vol'].zfill(2) == entry.name[:2] and (row['chap'].zfill(2) == str(float(re.search(r'_(.*?)_', entry.name).group(1))) or row['chap'].zfill(2) == re.search(r'_(.*?)_', entry.name).group(1)):\n",
    "                    #print(entry.name, 'passed')\n",
    "                    # find the relevant text file\n",
    "                    with open(entry.path, 'r', encoding=\"utf8\") as f:\n",
    "                        text = f.read()\n",
    "                        expl = 'expl_y' if row['expl'] == 'v' else 'expl_n'\n",
    "                        if row['enc'] == 'v' and row['expl'] != 'v':\n",
    "                            enc = 'enc_full'\n",
    "                        elif row['enc'] == 'v':\n",
    "                            enc = 'enc_part'\n",
    "                        elif row['enc_min'] == 'v':\n",
    "                            enc = 'enc_min'\n",
    "                        else:\n",
    "                            enc = 'enc_none'\n",
    "                        # determine name factors\n",
    "                        name = row['vol'].zfill(2) + '_' + row['chap'].zfill(2) + '-' + row['date'] + '-' + row['nat'] + '-' + enc + '-' + expl + '-' + row['title'] + '-' + row['pages'].replace('-', '_') + '.txt'\n",
    "                        # string formatted filename\n",
    "                        with open((new_path + '/' + name), 'w', encoding=\"utf8\") as fw:\n",
    "                            fw.write(text)\n",
    "                            #print('text outputted')\n",
    "                    #break\n",
    "                            \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df28a401-2846-4aa0-8bc1-f01d51be0586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "punctuation = [i for i in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "089c6e7a-985a-45cd-8867-d55b86ac8c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vol(name):\n",
    "    return(name[:2])\n",
    "def get_chap(name):\n",
    "    name = name[3:]\n",
    "    return(name[:name.find('_')])\n",
    "def wordcount(text):\n",
    "    '''splits texts on whitespace and counts the segments, ignoring punctuation'''\n",
    "    tokens = text.split()\n",
    "    words = [w for w in tokens if w not in punctuation]\n",
    "    return(len(words))\n",
    "def wordcount_f(vol, chap):\n",
    "    '''locates file, splits texts on whitespace and counts the segments, ignoring punctuation'''\n",
    "    filelist = os.scandir('text-data/CC_ML_FR_trimmed_morphad_lem')\n",
    "    for entry in filelist:\n",
    "        if int(entry.name[:2]) == int(vol):\n",
    "            if int(entry.name[3:5]) == int(chap):\n",
    "                with open(entry.path, mode = 'r', encoding=\"utf8\") as f:\n",
    "                    text = f.read()\n",
    "                    return(wordcount(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "76e73cc2-3574-481d-ae64-2f6126b83d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "testseg = '07_17.2_AMER_1170_The_most_ancient_voyage_and_discovery_of_the_West_Indies_performed_by_Madoc_the_sonne_of_Owen_Guined_pp.133-135.txt'\n",
    "testfull = '09_24_AMER_1572_The_memorable_voyage_of_M_John_Chilton_to_all_the_principall_parts_of_Nueva_Espanna_pp.360-377'\n",
    "#print(get_chap(test))\n",
    "print(testfull[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c8d09544-a538-4fbb-96b3-c0f0c5a66d71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "chap = '22.4'\n",
    "print(chap[:chap.find('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "303da1f9-194b-4541-aca4-33b7b173823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18963\n"
     ]
    }
   ],
   "source": [
    "missing_files = ['09_17.2_AMER_1540_The_voyage_of_the_right_worshipfull_knight_Francisco_de_Ulloa_pp.217-267.txt']\n",
    "missing_paths = ['text-data/CC_ML_FR_trimmed_MAlem_AMER_cat/' + f for f in missing_files]\n",
    "for path in missing_paths:\n",
    "    with open(path, mode = 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        print(wordcount(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58251fe9-e515-48e5-96f7-3578fe79d55d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2: prepare ledger for re-integration with rest of corpus\n",
    "\n",
    "1. compute & record segment wordcount\n",
    "1. [manually merge enc ledger with full ledger]\n",
    "1. compute & record full chapter wordcounts\n",
    "1. compute section fractions\n",
    "1. update the following fields from sections into parent chapters: national_perspective, exploration_component, intercultural_encounter_component, minor_intercultural_encounter_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d43f1c07-f0c6-4fdd-8584-5406aec942d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for 7, 23\n",
      "looking for 7, 30\n",
      "looking for 8, 14\n",
      "looking for 8, 49\n",
      "looking for 9, 1\n",
      "looking for 9, 2\n",
      "looking for 9, 15\n",
      "looking for 9, 17\n",
      "looking for 10, 9\n",
      "looking for 10, 23\n",
      "looking for 11, 20\n",
      "looking for 11, 25\n"
     ]
    }
   ],
   "source": [
    "cat_path = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat/'\n",
    "parent_path = 'text-data/CC_ML_FR_trimmed_morphad_lem'\n",
    "\n",
    "with open('text-data/ledger_merged_1.csv', mode = 'r') as ledger_csv:\n",
    "    full_ledger = csv.DictReader(ledger_csv) \n",
    "    with open('text-data/AMledger_wordcounts.csv', mode = 'r') as ledger2_csv:\n",
    "        AM_ledger = csv.DictReader(ledger2_csv)\n",
    "        #writing header row\n",
    "        with open('text-data/ledger_merged_2.csv', mode = 'a', newline='') as ledger_m2_csv:\n",
    "            writer = csv.DictWriter(ledger_m2_csv, ledger.fieldnames)\n",
    "            writer.writer.writerow(ledger.fieldnames)\n",
    "            #copy over all non-Am rows from full ledger + wordcounts + sect_f = 1 (includes header #2)\n",
    "            for row in full_ledger:\n",
    "                if row['geog'] != 'AMER':\n",
    "                    #except header # 2 from wordcount/sect\n",
    "                    if row['vol'] != 'volume':\n",
    "                        row['word'] = wordcount_f(row['vol'], row['chap'])\n",
    "                        row['sect_frac'] = 1\n",
    "                    writer.writerow(row)\n",
    "            #for Am rows:\n",
    "            #for full chapters in am ledger, just copy am rows into the new ledgers  + sect_f = 1\n",
    "            #when catches section: find corresponding full chapter;  + wordcount + sect_f = 1\n",
    "            # then write in all the sectioned rows while calculating fractions\n",
    "            #starting scan on am_ledger\n",
    "            for row in AM_ledger:\n",
    "                #for non-header rows:\n",
    "                if row['geog'] == 'AMER':\n",
    "                    if '.' not in row['chap']:\n",
    "                        #for full chapters in am ledger, just copy am rows into the new ledgers +  sect_f = 1\n",
    "                        filelist = os.scandir(parent_path)\n",
    "                        row['sect_frac'] = 1\n",
    "                        writer.writerow(row)\n",
    "                    else:\n",
    "                        #sequence of sections begins:  find corresponding full chapter row, write in + wordcount + sect_f = 1\n",
    "                                                    # then write in all the sectioned rows while calculating fractions\n",
    "                        #print(row)\n",
    "                        if row['chap'][-1] == '1':\n",
    "                            #bring over row from ful ledger\n",
    "                            vol = row['vol']\n",
    "                            chap = row['chap'][:row['chap'].find('.')]\n",
    "                            print(f'looking for {vol}, {chap}')\n",
    "                            with open('text-data/ledger_merged_1.csv', mode = 'r') as ledger3_csv:\n",
    "                                full_ledger2 = csv.DictReader(ledger3_csv) \n",
    "                                for row_f in full_ledger2:\n",
    "                                    #print(row_f)\n",
    "                                    if row_f['vol'] != 'volume':\n",
    "                                        if int(row_f['vol']) == int(vol) and int(row_f['chap']) == int(chap):\n",
    "                                            wordcount_full = wordcount_f(vol, chap)\n",
    "                                            row_f['word'] = wordcount_full\n",
    "                                            row_f['sect_frac'] = 1\n",
    "                                            writer.writerow(row_f)\n",
    "                                            break\n",
    "                        row['sect_frac'] = int(row['word']) / wordcount_full\n",
    "                        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c48517-d7b9-46f2-b353-6765c2cce445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
