{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5087a196-b3f2-4dc9-be8e-f5b0427d75bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text-cleaning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c3087-8180-4228-bdc2-14bc1a5c240e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Routines to go through text and correct misspellings / OCR errors\n",
    "\n",
    "Updating text throughout the 'trimmed' corpus tree.\n",
    "\n",
    "a global change will require scrubbing through (text-data/):\n",
    "\n",
    "- CC_ML_FR_trimmed_morphad_lem\n",
    "- CC_ML_FR_trimmed_morphad_lem_AMER\n",
    "- CC_ML_FR_trimmed_MAlem_AMER_cat_mod\n",
    "- CC_ML_FR_trimmed_MAlem_AMER_cat_mod_enc_full_part\n",
    "\n",
    "An American change will require scrubbing through the American portion of the first two and then through all the rest.\n",
    "\n",
    "Depending on testing, the unlemmatized **Cambridge_MacLehose_FineReader_OCR_trimmed** may be treated as part of global or separately\n",
    "\n",
    "To identify words, priorities & uses, I will need textcollections and freqdists for raw, total and AMER slices\n",
    "\n",
    "If I decide on a programmatic scrub (~for X top tokens, collapse everything within cloze fuzzy range into main token), I'll probably want to apply it without filtering stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c9942c-9311-40d5-ad6b-dddd9315207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process \n",
    "\n",
    "root = 'text-data/'\n",
    "\n",
    "am_root = root + 'CC_ML_FR_trimmed_morphad_lem_AMER/'\n",
    "am_cat_root = root + 'CC_ML_FR_trimmed_MAlem_AMER_cat'\n",
    "raw_root = root + 'Cambridge_MacLehose_FineReader_OCR_trimmed'\n",
    "tot_root = root + 'CC_ML_FR_trimmed_morphad_lem'\n",
    "trim_root = root + 'Cambridge_MacLehose_FineReader_OCR_trimmed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6eb230-be3c-4b40-a713-6cfe17f5d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "\n",
    "eliz_stopwords = [\"i\",  \"me\",  \"my\",  \"myself\",  \"we\",  \"our\",  \"ours\",  \"ourselves\",  \"you\",  \"your\",  \"yours\",  \"yourself\",  \"yourselves\",  \"he\",  \"him\",  \"his\",  \"himself\",  \"she\",  \"her\",  \"hers\",  \"herself\",  \"it\",  \"its\",  \"itself\",  \"they\",  \"them\",  \"their\",  \"theirs\",  \"themselves\",  \"what\",  \"which\",  \"who\",  \"whom\",  \"this\",  \"that\",  \"these\",  \"those\",  \"am\",  \"is\",  \"are\",  \"was\",  \"were\",  \"be\",  \"been\",  \"being\",  \"have\",  \"has\",  \"had\",  \"having\",  \"do\",  \"does\",  \"did\",  \"doing\",  \"a\",  \"an\",  \"the\",  \"and\",  \"but\",  \"if\",  \"or\",  \"because\",  \"as\",  \"until\",  \"while\",  \"of\",  \"at\",  \"by\",  \"for\",  \"with\",  \"about\",  \"against\",  \"between\",  \"into\",  \"through\",  \"during\",  \"before\",  \"after\",  \"above\",  \"below\",  \"to\",  \"from\",  \"up\",  \"down\",  \"in\",  \"out\",  \"o\",  \"on\",  \"off\",  \"over\",  \"under\",  \"again\",  \"further\",  \"then\",  \"once\",  \"here\",  \"there\",  \"when\",  \"where\",  \"why\",  \"how\",  \"all\",  \"any\",  \"both\",  \"each\",  \"few\",  \"more\",  \"most\",  \"other\",  \"some\",  \"such\",  \"no\",  \"nor\",  \"not\",  \"only\",  \"own\",  \"same\",  \"so\",  \"than\",  \"too\",  \"very\",  \"can\",  \"will\",  \"just\",  \"should\",  \"now\",  \"art\", \"doth\", \"dost\", \"'ere\", \"hast\", \"hath\", \"hence\", \"hither\", \"nigh\", \"oft\", \"should'st\", \"thither\", \"thee\", \"thou\", \"thine\", \"thy\", \"'tis\", \"'twas\", \"wast\", \"whence\", \"wherefore\", \"whereto\", \"withal\", \"would'st\", \"ye\", \"yon\", \"yonder\"]\n",
    "hk_stopwords = ['unto','u','one', 'five','upon','de','also','wee','two','may','many','would','shall','hee','like','three','doe','could','much','every','againe','bee','might','without','well','within','yet','bene','ad','foure','another','whereof','thereof','onely','next','himselfe','thus','hundred','untill','therefore','halfe','cum','selfe','non','ut', 'whole','little','sixe','full','neither','among','last','c','never','la','qui','ii','according','eight','whose','either','per','along','item','al','likewise','mee','whereupon','none','till','able','thousand','self','el','second','que','mine','quae','sunt','et','seven','iii','although','litle','si','notwithstanding','besides','etiam','lesse','e','even','vel','alwayes', 'third','ever','rather','whether','still','otherwise','large','amongst', 'greater','somewhat','ex','least','aforesaid','though','whatsoever','quam', 'ten','whereby','foorth','no', 'n','los','almost','twelve','howbeit','j', 'greatly','ac','yce', 'pro','en','ab','greatest','whereas','hoc','w','beene','doeth','eorum','con','withall','hereafter','moreover','nec','nine','noone','omnes','del','enim','often']\n",
    "latin_stopwords = ['ab', 'ac', 'ad', 'adhuc', 'aliqui', 'aliquis', 'an', 'ante', 'apud', 'at', 'atque', 'aut', 'autem', 'cum', 'cur', 'de', 'deinde', 'dum', 'ego', 'enim', 'ergo', 'es', 'est', 'et', 'etiam', 'etsi', 'ex', 'fio', 'haud', 'hic', 'iam', 'idem', 'igitur', 'ille', 'in', 'infra', 'inter', 'interim', 'ipse', 'is', 'ita', 'magis', 'modo', 'mox', 'nam', 'ne', 'nec', 'necque', 'neque', 'nisi', 'non', 'nos', 'o', 'ob', 'per', 'possum', 'post', 'pro', 'quae', 'quam', 'quare', 'qui', 'quia', 'quicumque', 'quidem', 'quilibet', 'quis', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quo', 'quoniam', 'sed', 'si', 'sic', 'sive', 'sub', 'sui', 'sum', 'super', 'suus', 'tam', 'tamen', 'trans', 'tu', 'tum', 'ubi', 'uel', 'uero']\n",
    "pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'you', 'they', 'me', 'you', 'him', 'her', 'it', 'you', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'your', 'their', 'mine', 'yours', 'his', 'hers', 'its', 'ours', 'yours', 'theirs', 'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'yourselves', 'themselves']\n",
    "modals = ['shall','shal','shalt','should', 'can', \"can't\", 'cannot', 'could', 'will','wil', 'would', 'may', 'must', 'might', 'ought', 'need', 'have', 'has']\n",
    "directives = ['without', 'within', 'there', 'thence','away','est','towards','toward','farre','betweene','wherein','therein']\n",
    "stop_words = set(stopwords.words('english') + latin_stopwords + eliz_stopwords + hk_stopwords + ['']) - set(pronouns + modals + directives)\n",
    "stop_words_max = set(stopwords.words('english') + latin_stopwords + eliz_stopwords + hk_stopwords + [''] + pronouns + modals + directives + ['great', 'make', 'good', 'part', 'certain', 'thing', 'wherewith', 'afterward', 'day'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce81770e-f3ba-4549-9c83-5a952c9ab79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whose care hath bene more generali NEW. And how may it be thought ;NEW, generali? generaliii\n"
     ]
    }
   ],
   "source": [
    "# #testing\n",
    "# s = 'whose care hath bene more generali 5people. And how may it be thought ;5people, generali? generaliii'\n",
    "# old_test = '5people'\n",
    "# new_test = 'NEW'\n",
    "# print(re.sub(r'([^a-zA-Z]|^)('+ old_test + r')([^a-zA-Z])',r'\\1' + new_test +r'\\3', s, flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "185b4b04-dfe6-4c2d-811b-7c68a9076e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(old, new, scope):\n",
    "    '''\n",
    "    replaces all instances of 'old' token with 'new' token across corpuses defined through 'scope'\n",
    "    arguments:\n",
    "        old (str) : token to be replaced\n",
    "        new (str) : substitute token\n",
    "        scope (str) : \n",
    "            'am' : American materials\n",
    "            'tot' : all lemmatized materials\n",
    "            'raw' : unlemmatized (complete collection)\n",
    "            'totnraw' : both lemmatized and unlemmatized folders '''\n",
    "    #since all but 'raw' scope includes the regrettably fragmented American materials, pre-load them \n",
    "    folders = [am_root, am_cat_root]\n",
    "    if scope == 'raw':\n",
    "        folders = [raw_root]\n",
    "    elif scope == 'am':\n",
    "        folders += []\n",
    "    elif scope == 'tot':\n",
    "        folders += [tot_root]\n",
    "    elif scope == 'totnraw':\n",
    "        folders += [tot_root, raw_root]\n",
    "    elif scope == 'test':\n",
    "        folders = [root + 'test/']\n",
    "    else:\n",
    "        raise ValueError('unrecognized scope argument')\n",
    "    #print(folders)\n",
    "    for folder in folders:\n",
    "            filelist = os.scandir(folder)\n",
    "            for entry in filelist:\n",
    "                with open(entry.path, 'r', encoding=\"utf8\") as fr:\n",
    "                        text = fr.read()\n",
    "                        # replacement through regex to account for adjacent punctuation & ignore case\n",
    "                        # regex fails if 'new' starts with a number; prefacing the number with a space gets around that\n",
    "                        text = re.sub(r'([^a-zA-Z]|^)('+ old + r')([^a-zA-Z])',r'\\1' + new +r'\\3', text, flags=re.I)\n",
    "                with open(entry.path, 'w', encoding=\"utf8\") as fw:\n",
    "                        fw.write(text)\n",
    "    with open (root + 'replacement_record.txt', 'a', encoding=\"utf8\") as f:\n",
    "        f.write(old + ' -> ' + new + ' in ' + scope + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a6515cad-0421-45e5-8b85-ead10256f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra layer of cleaning, including placeholder newlines for morphadorner\n",
    "filelist = os.scandir('text-data/CC_ML_FR_trimmed_cleaned_EN/')\n",
    "for entry in filelist:\n",
    "    with open(entry.path, 'r', encoding=\"utf8\") as fr:\n",
    "        text = fr.read()\n",
    "        text = text.replace('THE ENGLISH VOYAGES', '')\n",
    "        text = re.sub(r'\\[.{1,9}?\\]', '', text)\n",
    "        text = re.sub(r'\\n+', r'{n}', text)\n",
    "        with open('text-data/CC_ML_FR_trimmed_cleaned_EN_cl/' + entry.name, 'w', encoding=\"utf8\") as fw:\n",
    "            fw.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b2701e4-fcff-4358-a86d-351739b047b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning out the { n } in MA outputs\n",
    "folder = 'text-data/CC_ML_FR_trimmed_cleaned_EN_cl_MAlem'\n",
    "filelist = os.scandir(folder)\n",
    "for entry in filelist:\n",
    "    with open(entry.path, 'r+', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('{ n }', '\\n')\n",
    "        f.seek(0)\n",
    "        f.write(text)\n",
    "        f.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5e0db-cff1-410e-89a1-ed9ff435fd52",
   "metadata": {},
   "source": [
    "### Exploring targets for spelling standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a90214-f7f9-4e64-849f-547eecce432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpuses\n",
    "am = PlaintextCorpusReader(am_root, '.*')\n",
    "am_fd = nltk.FreqDist(word for word in am.words() if word.isalpha() and word not in stop_words)\n",
    "am_col = nltk.TextCollection(am)\n",
    "\n",
    "raw = PlaintextCorpusReader(raw_root, '.*')\n",
    "raw_fd = nltk.FreqDist(word for word in raw.words() if word.isalpha() and word not in stop_words)\n",
    "raw_col = nltk.TextCollection(raw)\n",
    "\n",
    "tot = PlaintextCorpusReader(tot_root, '.*')\n",
    "tot_fd = nltk.FreqDist(word for word in tot.words() if word.isalpha() and word not in stop_words)\n",
    "tot_col = nltk.TextCollection(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d62d18f3-2194-45e2-ab62-8868826c3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing\n",
    "# with open('text-data/test/11_30_AMER_1587_The_prosperous_voyage_of_M_Thomas_Candish_esquire_into_the_South_sea_and_so_round_about_the_circumfe_pp.290-347.txt', 'r', encoding=\"utf8\") as fr:\n",
    "#     text = fr.read()\n",
    "#     text = text.replace('THE ENGLISH VOYAGES', '')\n",
    "#     text = re.sub(r'\\[.{1,9}?\\]', '', text)\n",
    "#     text = re.sub(r'\\n+', r'{n}', text)\n",
    "#     print(text)\n",
    "\n",
    "# test = \"The state of the shipping of the Cinque ports from Edward the Confessor and William the Conqueror , and so down to Edward the first , faithfully gathered by the learned Gentleman 42 { n } M. Willaim Lambert in his Perambulation of Kent , out of the most ancient Records of England . { n } Find in the book of the general ! survey The antiquity of the Realm , which William the Con - °fthe Porls-querour caused to be made in the fourth IO7°-yeere of his reign , and to be called Domesday , because ( as Matthew Parise says ) it spared no man but judged all men indifferently , as the Lord in that great { n } day will do , that Dover , Sandwich , and Rumney , were in the time of K. Edward the Confessor , discharged almost of all manner of impositions and burdens ( which other towns did bear ) in consideration of such service to be done by them upon the Sea , as in their special titles shall hereafter appear . { n } Whereupon , although I might ground reasonable conjecture , that the immunity of the haven Towns ( which we now call by a certain number , the Cinque Ports ) might take their beginning from the same Edward : yet for as much as I read in the Chartre of K. Edward the first after the conquest ( which is reported in our book of Entries ) A reci tall of the grants of sundry kings to the Five Ports , the same reaching no higher then to William the Conqueror , I will leave my conjecture , and lean to his Chartre : contenting my self to yield to the Conqueror , the thanks of other men's benefits , seeing those which were benefited , were wisely contented ( as the case then stood ) to like better of his confirmation ( or second gift ) then of K. Edwards first grant , and endowment . { n } And to the end that I may proceed in some manner of array , I will first show , which Towns were at the beginning taken for the Five Ports , and what others be now reputed in the same number : secondly , what service they ought , and did in times passed : and lastly , what privileges they have therefore , and by what persons they have be governed .\"\n",
    "# print(test.replace('{ n }', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d393aed-7bf5-4625-8424-989d6597ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_flags = ['violence', 'harm', 'injury', 'injure', 'hurt', 'damage', 'scathe', 'wound', 'maim', 'cripple', 'mutilate', 'cut', 'mangle', 'torture', 'torment', 'wound', 'gash', 'bruise', 'abuse', \n",
    "              'bloody', 'bloodshed', 'bloodshedder', 'bloodshedding', 'blood', 'hit', \n",
    "              'fight', 'scrap', 'struggle', 'conflict', 'melee', 'brawl', 'combat', 'wrestle', 'wrestler',\n",
    "              'kill', 'death', 'slay', 'murder', 'assassinate', 'assasin', 'massacre', 'slaughter', 'butcher', 'slaughter', 'manslaughter', \n",
    "              'battle', 'war', 'siege', 'attack', 'assault', 'skirmish', 'skirmisher', 'enemy', 'foe', 'hostile', 'army', 'soldier', 'warrior', 'conquer', 'conqueror',  'conquest'\n",
    "              'detain', 'capture', 'captive', 'imprison', 'gaol', 'prisoner', 'slave', 'enslave',\n",
    "              'shoot', 'shot',  'blast', 'burn', 'fire', 'blaze',\n",
    "              'cruel', 'cruelty', 'destroy', \n",
    "              'arrow', 'crossbow', 'dart', 'javelin', 'mace', 'club', 'sword', 'lance', 'spear', 'rapier', 'pike', 'target', 'buckler', 'falchion', 'halberd', 'partisan', \n",
    "              'musket', 'gun', 'bullet', 'caliver', 'culverin', 'harquebus', 'harquebusier', 'saker', 'cannon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5495f276-5636-4bd3-ae08-229095078931",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_fd.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2193099e-cb27-4ecd-abdc-803584b8e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 4 of 4 matches:\n",
      " like to be starve , but the other false . nevertheless until my return it take such effect in pemisapans breast , and in those against we , that they grow not only into contempt of we , but also ( co\n",
      "e attempt to run away , i lay he in the bylboe , threaten to cut off his head , who i remit at pemisapans request : whereupon he be persuade that he be our enemy to the death , he do not only feed he \n",
      "ake much of he , he flat discover all unto i , which also afterward be reveal unto i by one of pemisapans own man , that night before he be slay . Theise mischief be all instant upon i and my company \n",
      "gligence to have be intercept by the savage , we meet he Pemisapan return out of the wood with pemisapans head in slay - he hand .. This fall out the first of June 1586 , and the eight of the same com\n"
     ]
    }
   ],
   "source": [
    "tot_col.concordance('pemisapans', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec41a70-2969-4ac7-931f-45af97723a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harquebus\n",
      "rebus\n",
      "harquebusier\n",
      "haeredibus\n",
      "Marques\n",
      "marquess\n",
      "arcubus\n",
      "haquinus\n",
      "harque\n",
      "hargubuser\n",
      "harebornus\n",
      "hargubuz\n",
      "harquebusiers\n",
      "Marquese\n",
      "harquebusses\n"
     ]
    }
   ],
   "source": [
    "# 86 threshold for 'people' 'harquebus', 'harquebuss',\n",
    "for word in tot_col.vocab():\n",
    "    if fuzz.ratio('harquebus', word) > 70:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71ee3fc-c3b8-457d-9797-a31147b64d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace('harquebuss','harquebus','totnraw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e917e1-cfdb-4181-b3f9-7d61ed5b4c29",
   "metadata": {},
   "source": [
    "## clearing out Latin text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a35924-c04c-47c1-b69c-91df3021a4a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### false starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7477b73-f4de-445b-b7e6-7955cc099262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# # First attempt - manual flagging\n",
    "# # with open(text-data/comaprison_texts/latin.txt, 'r+', encoding=\"utf8\") as f:\n",
    "# #     latin = f.read()\n",
    "    \n",
    "# latin = PlaintextCorpusReader('text-data/comparison_texts', 'latin.txt')\n",
    "# latin_fd = nltk.FreqDist(word for word in latin.words() if word.isalpha())\n",
    "# latin_fd.most_common(60) \n",
    "\n",
    "# latin_flags = ['ad', 'de', 'est', 'quod', 'cum', 'non', 'ut', 'qui', 'sunt', 'et', 'Et', 'etiam', 'quae', 'vel', 'autem', 'quam', 'eorum', 'sed', 'quia', 'eos', 'nos', 'enim', 'hoc', 'se', 'nec', 'ab', 'omnes', 'nobis', 'homines', 'vero', 'tamen', 'eis', 'esse', 'pro', 'habent', 'usque', 'terram', 'sicut', 'terra', 'De', 'illa', 'nisi', 'faciunt', 'quo', 'unum', 'quibus', 'habet', 'ita', 'ejus', 'ibi']\n",
    "# print(len(latin_flags))\n",
    "\n",
    "# filelist = os.scandir('text-data/CC_ML_FR_trimmed_cleaned')\n",
    "# lat_texts = 0\n",
    "# for entry in filelist:\n",
    "#     with open(entry.path, 'r', encoding=\"utf8\") as fr:\n",
    "#         text = fr.read()\n",
    "#         lat_count = 0\n",
    "#         for flag in latin_flags:\n",
    "#             if flag in text:\n",
    "#                 lat_count += 1\n",
    "#         if lat_count >= 15:\n",
    "#             print(entry.name)\n",
    "#             lat_texts += 1\n",
    "            \n",
    "# print(lat_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "542564a7-1542-4b92-ac53-5a61ced963a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "# # check 'the same in English' expression flag\n",
    "# filelist = os.scandir('text-data/CC_ML_FR_trimmed_cleaned')\n",
    "# sameng = 0\n",
    "# for entry in filelist:\n",
    "#     with open(entry.path, 'r', encoding=\"utf8\") as fr:\n",
    "#         text = fr.read()\n",
    "#         if 'same in English' in text:\n",
    "#             sameng+=1\n",
    "# print(sameng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e33bbf7-cf59-4e9f-9ab0-82dec41cec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #second attempt through TextBlob's language detection\n",
    "# from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea473436-03af-4b57-a9af-3a30d3984ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('text-data/Cambridge_MacLehose_FineReader_OCR_trimmed/01_01_NNE-_0517_The_voyage_of_Arthur_K_of_Britaine_to_Island_and_the_most_Northeastern_parts_of_Europe_Anno_517_pp.003-004.txt', 'r', encoding=\"utf8\") as fr:\n",
    "#     text = fr.read()\n",
    "# sents = text.split('\\n')\n",
    "# for sent in sents:\n",
    "#     lang = TextBlob(sent)\n",
    "#     print(lang)\n",
    "#     print(lang.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0aba8eee-d194-4b58-8fb6-ef3d56e75910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang = TextBlob('bob')\n",
    "# print(lang)\n",
    "# print(lang.detect_language())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06470327-60ac-4987-8aa7-e0baf6a976fb",
   "metadata": {},
   "source": [
    "### langid implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4215a271-6f94-4708-9833-5f5eddcff23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "identifier.set_languages(['en','la','es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26c817ca-8e57-48b1-b170-409b4639506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #test runs\n",
    "# with open('text-data/CC_ML_FR_trimmed_cleaned/11_38_AMER_1596_The_letters_of_the_Queenes_most_excellent_Majestie_sent_in_the_yere_1596_to_the_emperour_of_China_by_pp.417-421.txt', 'r', encoding=\"utf8\") as fr:\n",
    "#     text = fr.read()\n",
    "# sents = text.split('{n}')\n",
    "# for sent in sents:\n",
    "#     if len(sent) > 2:\n",
    "#         print(sent)\n",
    "#         print(langid.classify(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1bec1b32-0e90-4694-b094-7f57db65d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumps non-EN text\n",
    "source = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat'\n",
    "target = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat_EN/'\n",
    "filelist = os.scandir(source)\n",
    "for entry in filelist:\n",
    "    with open(entry.path, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        sents = text.split('{n}')\n",
    "        EN_sents = []\n",
    "        for sent in sents:\n",
    "            if langid.classify(sent)[0] == 'en':\n",
    "                EN_sents.append(sent)\n",
    "        EN_text = '{n}'.join(EN_sents)\n",
    "        # control for small bits of English misidentified\n",
    "        if len(EN_text) > 0.7* len(text):\n",
    "            EN_text = text\n",
    "        with open(target + entry.name, 'w', encoding=\"utf8\") as fw:\n",
    "            fw.write(EN_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28cef7-3c12-4b36-87c1-897f06dc6a10",
   "metadata": {},
   "source": [
    "**having created a base EN corpus, I will want to create the following:**\n",
    "- ~EN version of CC_ML_FR_trimmed_MAlem_AMER_cat~\n",
    "- ~MorphAdorn the base EN~\n",
    "  - ~from that, create lem and spel versions; convert {n} to newlines~\n",
    "- recreate ledger with just EN wordcounts; run viol ratios\n",
    "- create AM lem folder\n",
    "- MALLET general lem / AM lem at higher topic numbers in case it gets me a fighting topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
