{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a8d9aa-0144-402c-af5c-b32598ab5400",
   "metadata": {
    "tags": []
   },
   "source": [
    "# routines to go through text and correct misspellings / OCR errors\n",
    "\n",
    "Updating text throughout the 'trimmed' corpus tree.\n",
    "\n",
    "a global change will require scrubbing through (text-data/):\n",
    "\n",
    "- CC_ML_FR_trimmed_morphad_lem\n",
    "- CC_ML_FR_trimmed_morphad_lem_AMER\n",
    "- CC_ML_FR_trimmed_MAlem_AMER_cat_mod\n",
    "- CC_ML_FR_trimmed_MAlem_AMER_cat_mod_enc_full_part\n",
    "\n",
    "An American change will require scrubbing through the American portion of the first two and then through all the rest.\n",
    "\n",
    "Depending on testing, the unlemmatized **Cambridge_MacLehose_FineReader_OCR_trimmed** may be treated as part of global or separately\n",
    "\n",
    "To identify words, priorities & uses, I will need textcollections and freqdists for raw, total and AMER slices\n",
    "\n",
    "If I decide on a programmatic scrub (~for X top tokens, collapse everything within cloze fuzzy range into main token), I'll probably want to apply it without filtering stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88c9942c-9311-40d5-ad6b-dddd9315207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process \n",
    "\n",
    "root = 'text-data/'\n",
    "\n",
    "am_root = root + 'CC_ML_FR_trimmed_morphad_lem_AMER/'\n",
    "am_cat_root = root + 'CC_ML_FR_trimmed_MAlem_AMER_cat_mod'\n",
    "am_enc_root = root + 'CC_ML_FR_trimmed_MAlem_AMER_cat_mod_enc_full_part'\n",
    "raw_root = root + 'Cambridge_MacLehose_FineReader_OCR_trimmed'\n",
    "tot_root = root + 'CC_ML_FR_trimmed_morphad_lem'\n",
    "trim_root = root + 'Cambridge_MacLehose_FineReader_OCR_trimmed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b6eb230-be3c-4b40-a713-6cfe17f5d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "\n",
    "eliz_stopwords = [\"i\",  \"me\",  \"my\",  \"myself\",  \"we\",  \"our\",  \"ours\",  \"ourselves\",  \"you\",  \"your\",  \"yours\",  \"yourself\",  \"yourselves\",  \"he\",  \"him\",  \"his\",  \"himself\",  \"she\",  \"her\",  \"hers\",  \"herself\",  \"it\",  \"its\",  \"itself\",  \"they\",  \"them\",  \"their\",  \"theirs\",  \"themselves\",  \"what\",  \"which\",  \"who\",  \"whom\",  \"this\",  \"that\",  \"these\",  \"those\",  \"am\",  \"is\",  \"are\",  \"was\",  \"were\",  \"be\",  \"been\",  \"being\",  \"have\",  \"has\",  \"had\",  \"having\",  \"do\",  \"does\",  \"did\",  \"doing\",  \"a\",  \"an\",  \"the\",  \"and\",  \"but\",  \"if\",  \"or\",  \"because\",  \"as\",  \"until\",  \"while\",  \"of\",  \"at\",  \"by\",  \"for\",  \"with\",  \"about\",  \"against\",  \"between\",  \"into\",  \"through\",  \"during\",  \"before\",  \"after\",  \"above\",  \"below\",  \"to\",  \"from\",  \"up\",  \"down\",  \"in\",  \"out\",  \"o\",  \"on\",  \"off\",  \"over\",  \"under\",  \"again\",  \"further\",  \"then\",  \"once\",  \"here\",  \"there\",  \"when\",  \"where\",  \"why\",  \"how\",  \"all\",  \"any\",  \"both\",  \"each\",  \"few\",  \"more\",  \"most\",  \"other\",  \"some\",  \"such\",  \"no\",  \"nor\",  \"not\",  \"only\",  \"own\",  \"same\",  \"so\",  \"than\",  \"too\",  \"very\",  \"can\",  \"will\",  \"just\",  \"should\",  \"now\",  \"art\", \"doth\", \"dost\", \"'ere\", \"hast\", \"hath\", \"hence\", \"hither\", \"nigh\", \"oft\", \"should'st\", \"thither\", \"thee\", \"thou\", \"thine\", \"thy\", \"'tis\", \"'twas\", \"wast\", \"whence\", \"wherefore\", \"whereto\", \"withal\", \"would'st\", \"ye\", \"yon\", \"yonder\"]\n",
    "hk_stopwords = ['unto','u','one', 'five','upon','de','also','wee','two','may','many','would','shall','hee','like','three','doe','could','much','every','againe','bee','might','without','well','within','yet','bene','ad','foure','another','whereof','thereof','onely','next','himselfe','thus','hundred','untill','therefore','halfe','cum','selfe','non','ut', 'whole','little','sixe','full','neither','among','last','c','never','la','qui','ii','according','eight','whose','either','per','along','item','al','likewise','mee','whereupon','none','till','able','thousand','self','el','second','que','mine','quae','sunt','et','seven','iii','although','litle','si','notwithstanding','besides','etiam','lesse','e','even','vel','alwayes', 'third','ever','rather','whether','still','otherwise','large','amongst', 'greater','somewhat','ex','least','aforesaid','though','whatsoever','quam', 'ten','whereby','foorth','no', 'n','los','almost','twelve','howbeit','j', 'greatly','ac','yce', 'pro','en','ab','greatest','whereas','hoc','w','beene','doeth','eorum','con','withall','hereafter','moreover','nec','nine','noone','omnes','del','enim','often']\n",
    "latin_stopwords = ['ab', 'ac', 'ad', 'adhuc', 'aliqui', 'aliquis', 'an', 'ante', 'apud', 'at', 'atque', 'aut', 'autem', 'cum', 'cur', 'de', 'deinde', 'dum', 'ego', 'enim', 'ergo', 'es', 'est', 'et', 'etiam', 'etsi', 'ex', 'fio', 'haud', 'hic', 'iam', 'idem', 'igitur', 'ille', 'in', 'infra', 'inter', 'interim', 'ipse', 'is', 'ita', 'magis', 'modo', 'mox', 'nam', 'ne', 'nec', 'necque', 'neque', 'nisi', 'non', 'nos', 'o', 'ob', 'per', 'possum', 'post', 'pro', 'quae', 'quam', 'quare', 'qui', 'quia', 'quicumque', 'quidem', 'quilibet', 'quis', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quo', 'quoniam', 'sed', 'si', 'sic', 'sive', 'sub', 'sui', 'sum', 'super', 'suus', 'tam', 'tamen', 'trans', 'tu', 'tum', 'ubi', 'uel', 'uero']\n",
    "pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'you', 'they', 'me', 'you', 'him', 'her', 'it', 'you', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'your', 'their', 'mine', 'yours', 'his', 'hers', 'its', 'ours', 'yours', 'theirs', 'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'yourselves', 'themselves']\n",
    "modals = ['shall','shal','shalt','should', 'can', \"can't\", 'cannot', 'could', 'will','wil', 'would', 'may', 'must', 'might', 'ought', 'need', 'have', 'has']\n",
    "directives = ['without', 'within', 'there', 'thence','away','est','towards','toward','farre','betweene','wherein','therein']\n",
    "stop_words = set(stopwords.words('english') + latin_stopwords + eliz_stopwords + hk_stopwords + ['']) - set(pronouns + modals + directives)\n",
    "stop_words_max = set(stopwords.words('english') + latin_stopwords + eliz_stopwords + hk_stopwords + [''] + pronouns + modals + directives + ['great', 'make', 'good', 'part', 'certain', 'thing', 'wherewith', 'afterward', 'day'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce81770e-f3ba-4549-9c83-5a952c9ab79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whose care hath bene more generali NEW. And how may it be thought ;NEW, generali? generaliii\n"
     ]
    }
   ],
   "source": [
    "# #testing\n",
    "# s = 'whose care hath bene more generali 5people. And how may it be thought ;5people, generali? generaliii'\n",
    "# old_test = '5people'\n",
    "# new_test = 'NEW'\n",
    "# print(re.sub(r'([^a-zA-Z]|^)('+ old_test + r')([^a-zA-Z])',r'\\1' + new_test +r'\\3', s, flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185b4b04-dfe6-4c2d-811b-7c68a9076e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(old, new, scope):\n",
    "    '''\n",
    "    replaces all instances of 'old' token with 'new' token across corpuses defined through 'scope'\n",
    "    arguments:\n",
    "        old (str) : token to be replaced\n",
    "        new (str) : substitute token\n",
    "        scope (str) : \n",
    "            'am' : American materials\n",
    "            'tot' : all lemmatized materials\n",
    "            'raw' : unlemmatized (complete collection)\n",
    "            'totnraw' : both lemmatized and unlemmatized folders '''\n",
    "    #since all but 'raw' scope includes the regrettably fragmented American materials, pre-load them \n",
    "    folders = [am_root, am_cat_root, am_enc_root]\n",
    "    if scope == 'raw':\n",
    "        folders = [raw_root]\n",
    "    elif scope == 'am':\n",
    "        folders += []\n",
    "    elif scope == 'tot':\n",
    "        folders += [tot_root]\n",
    "    elif scope == 'totnraw':\n",
    "        folders += [tot_root, raw_root]\n",
    "    elif scope == 'test':\n",
    "        folders = [root + 'test/']\n",
    "    else:\n",
    "        raise ValueError('unrecognized scope argument')\n",
    "    #print(folders)\n",
    "    for folder in folders:\n",
    "            filelist = os.scandir(folder)\n",
    "            for entry in filelist:\n",
    "                with open(entry.path, 'r', encoding=\"utf8\") as fr:\n",
    "                        text = fr.read()\n",
    "                        # replacement through regex to account for adjacent punctuation & ignore case\n",
    "                        # regex fails if 'new' starts with a number; prefacing the number with a space gets around that\n",
    "                        text = re.sub(r'([^a-zA-Z]|^)('+ old + r')([^a-zA-Z])',r'\\1' + new +r'\\3', text, flags=re.I)\n",
    "                with open(entry.path, 'w', encoding=\"utf8\") as fw:\n",
    "                        fw.write(text)\n",
    "    with open (root + 'replacement_record.txt', 'a', encoding=\"utf8\") as f:\n",
    "        f.write(old + ' -> ' + new + ' in ' + scope + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f27a2249-afa1-476d-8def-1d1b244a79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up source text from Cambridge_MacLehose_FineReader_OCR_trimmed corpus\n",
    "filelist = os.scandir(trim_root)\n",
    "for entry in filelist:\n",
    "    with open(entry.path, 'r', encoding=\"utf8\") as fr:\n",
    "        text = fr.read()\n",
    "        text = text.replace('THE ENGLISH VOYAGES', '')\n",
    "        text = re.sub(r'\\[.{1,9}?\\]', '', text)\n",
    "        text = re.sub(r'\\n+', r'{n}', text)\n",
    "        with open('text-data/CC_ML_FR_trimmed_cleaned/' + entry.name, 'w', encoding=\"utf8\") as fw:\n",
    "            fw.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "715087bb-1d83-4e7d-af3c-2ab3440a3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning out the { n } in MA_spel\n",
    "folder = 'text-data/CC_ML_FR_trimmed_cleaned_MAspel'\n",
    "filelist = os.scandir(folder)\n",
    "for entry in filelist:\n",
    "    with open(entry.path, 'r+', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('{ n }', '\\n')\n",
    "        f.seek(0)\n",
    "        f.write(text)\n",
    "        f.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d62d18f3-2194-45e2-ab62-8868826c3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing\n",
    "# with open('text-data/test/11_30_AMER_1587_The_prosperous_voyage_of_M_Thomas_Candish_esquire_into_the_South_sea_and_so_round_about_the_circumfe_pp.290-347.txt', 'r', encoding=\"utf8\") as fr:\n",
    "#     text = fr.read()\n",
    "#     text = text.replace('THE ENGLISH VOYAGES', '')\n",
    "#     text = re.sub(r'\\[.{1,9}?\\]', '', text)\n",
    "#     text = re.sub(r'\\n+', r'{n}', text)\n",
    "#     print(text)\n",
    "\n",
    "# test = \"The state of the shipping of the Cinque ports from Edward the Confessor and William the Conqueror , and so down to Edward the first , faithfully gathered by the learned Gentleman 42 { n } M. Willaim Lambert in his Perambulation of Kent , out of the most ancient Records of England . { n } Find in the book of the general ! survey The antiquity of the Realm , which William the Con - °fthe Porls-querour caused to be made in the fourth IO7°-yeere of his reign , and to be called Domesday , because ( as Matthew Parise says ) it spared no man but judged all men indifferently , as the Lord in that great { n } day will do , that Dover , Sandwich , and Rumney , were in the time of K. Edward the Confessor , discharged almost of all manner of impositions and burdens ( which other towns did bear ) in consideration of such service to be done by them upon the Sea , as in their special titles shall hereafter appear . { n } Whereupon , although I might ground reasonable conjecture , that the immunity of the haven Towns ( which we now call by a certain number , the Cinque Ports ) might take their beginning from the same Edward : yet for as much as I read in the Chartre of K. Edward the first after the conquest ( which is reported in our book of Entries ) A reci tall of the grants of sundry kings to the Five Ports , the same reaching no higher then to William the Conqueror , I will leave my conjecture , and lean to his Chartre : contenting my self to yield to the Conqueror , the thanks of other men's benefits , seeing those which were benefited , were wisely contented ( as the case then stood ) to like better of his confirmation ( or second gift ) then of K. Edwards first grant , and endowment . { n } And to the end that I may proceed in some manner of array , I will first show , which Towns were at the beginning taken for the Five Ports , and what others be now reputed in the same number : secondly , what service they ought , and did in times passed : and lastly , what privileges they have therefore , and by what persons they have be governed .\"\n",
    "# print(test.replace('{ n }', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a90214-f7f9-4e64-849f-547eecce432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpuses\n",
    "am = PlaintextCorpusReader(am_root, '.*')\n",
    "am_fd = nltk.FreqDist(word for word in am.words() if word.isalpha() and word not in stop_words)\n",
    "am_col = nltk.TextCollection(am)\n",
    "\n",
    "raw = PlaintextCorpusReader(raw_root, '.*')\n",
    "raw_fd = nltk.FreqDist(word for word in raw.words() if word.isalpha() and word not in stop_words)\n",
    "raw_col = nltk.TextCollection(raw)\n",
    "\n",
    "tot = PlaintextCorpusReader(tot_root, '.*')\n",
    "tot_fd = nltk.FreqDist(word for word in tot.words() if word.isalpha() and word not in stop_words)\n",
    "tot_col = nltk.TextCollection(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d393aed-7bf5-4625-8424-989d6597ed51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\ndie is filtered out on account of dyes\\npain is sometimes related to legal pressure / coercion, but rarely to physical viol\\nruin doesn't quite apply to human bodies\\nagony just doesn't make an appearance\\nbow is frequently a bowing action, but arrow and shoot should cover it\\npiece can be many things\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# violence flags for pandas conditional formatting\n",
    "viol_flags = ['violence', 'harm', 'injury', 'injure', 'hurt', 'damage', 'scathe', 'wound', 'maim', 'cripple', 'mutilate', 'cut', 'mangle', 'torture', 'torment', 'wound', 'gash', 'bruise', 'abuse', \n",
    "              'bloody', 'bloodshed', 'bloodshedder', 'bloodshedding', 'blood', 'hit', \n",
    "              'fight', 'scrap', 'struggle', 'conflict', 'melee', 'brawl', 'combat', 'wrestle', 'wrestler',\n",
    "              'kill', 'death', 'slay', 'murder', 'assassinate', 'assasin', 'massacre', 'slaughter', 'butcher', 'slaughter', 'manslaughter', \n",
    "              'battle', 'war', 'siege', 'attack', 'assault', 'skirmish', 'skirmisher', 'enemy', 'foe', 'hostile', 'army', 'soldier', 'warrior', 'conquer', 'conqueror',  'conquest'\n",
    "              'detain', 'capture', 'captive', 'imprison', 'gaol', 'prisoner', 'slave', 'enslave',\n",
    "              'shoot', 'shot',  'blast', 'burn', 'fire', 'blaze',\n",
    "              'cruel', 'cruelty', 'destroy', \n",
    "              'arrow', 'crossbow', 'dart', 'javelin', 'mace', 'club', 'sword', 'lance', 'spear', 'rapier', 'pike', 'target', 'buckler', 'falchion', 'halberd', 'partisan', \n",
    "              'musket', 'gun', 'bullet', 'caliver', 'culverin', 'harquebus', 'harquebusier', 'saker', 'cannon']\n",
    "''' \n",
    "die is filtered out on account of dyes\n",
    "pain is sometimes related to legal pressure / coercion, but rarely to physical viol\n",
    "ruin doesn't quite apply to human bodies\n",
    "agony just doesn't make an appearance\n",
    "bow is frequently a bowing action, but arrow and shoot should cover it\n",
    "piece can be many things\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2193099e-cb27-4ecd-abdc-803584b8e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 4 of 4 matches:\n",
      " like to be starve , but the other false . nevertheless until my return it take such effect in pemisapans breast , and in those against we , that they grow not only into contempt of we , but also ( co\n",
      "e attempt to run away , i lay he in the bylboe , threaten to cut off his head , who i remit at pemisapans request : whereupon he be persuade that he be our enemy to the death , he do not only feed he \n",
      "ake much of he , he flat discover all unto i , which also afterward be reveal unto i by one of pemisapans own man , that night before he be slay . Theise mischief be all instant upon i and my company \n",
      "gligence to have be intercept by the savage , we meet he Pemisapan return out of the wood with pemisapans head in slay - he hand .. This fall out the first of June 1586 , and the eight of the same com\n"
     ]
    }
   ],
   "source": [
    "tot_col.concordance('pemisapans', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bec41a70-2969-4ac7-931f-45af97723a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pemisapan\n",
      "permians\n",
      "pemisapans\n",
      "embiavan\n",
      "mishapen\n",
      "fpemisa\n",
      "minsapa\n"
     ]
    }
   ],
   "source": [
    "# 86 threshold for 'people'\n",
    "for word in tot_col.vocab():\n",
    "    if fuzz.ratio('pemisapan', word) > 70:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c71ee3fc-c3b8-457d-9797-a31147b64d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace('naturali','natural','totnraw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2479fb8-a171-48a6-b0f8-1e882ba765e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('they', 28743),\n",
       " ('we', 23018),\n",
       " ('have', 18427),\n",
       " ('he', 15611),\n",
       " ('our', 12546),\n",
       " ('i', 12152),\n",
       " ('their', 11770),\n",
       " ('it', 11196),\n",
       " ('his', 10625),\n",
       " ('great', 8019),\n",
       " ('there', 7525),\n",
       " ('come', 7476),\n",
       " ('say', 6709),\n",
       " ('man', 6633),\n",
       " ('shall', 6015),\n",
       " ('ship', 5681),\n",
       " ('day', 5645),\n",
       " ('will', 5626),\n",
       " ('make', 5580),\n",
       " ('go', 4848),\n",
       " ('may', 4604),\n",
       " ('you', 4378),\n",
       " ('take', 4370),\n",
       " ('good', 4098),\n",
       " ('place', 3772),\n",
       " ('time', 3737),\n",
       " ('call', 3659),\n",
       " ('land', 3646),\n",
       " ('king', 3419),\n",
       " ('see', 3392),\n",
       " ('can', 3337),\n",
       " ('country', 3253),\n",
       " ('find', 3114),\n",
       " ('my', 2925),\n",
       " ('sea', 2918),\n",
       " ('part', 2914),\n",
       " ('river', 2724),\n",
       " ('give', 2588),\n",
       " ('your', 2547),\n",
       " ('certain', 2518),\n",
       " ('send', 2506),\n",
       " ('league', 2497),\n",
       " ('thing', 2489),\n",
       " ('captain', 2454),\n",
       " ('year', 2414),\n",
       " ('island', 2400),\n",
       " ('town', 2360),\n",
       " ('hand', 2336),\n",
       " ('bring', 2285),\n",
       " ('water', 2282)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_fd.most_common(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
