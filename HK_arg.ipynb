{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706db745-0c47-418e-b84b-c4afe2a96895",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e6d45f-3c08-4b79-b6fb-f880da968af3",
   "metadata": {},
   "source": [
    "Richard Hakluyt (1553-1616), though an ordained priest, is chefly remembered for his promotion of English colonial expansion, and especially for his printed works. He published *Divers Voyages Touching the Discoverie of America* in 1582; the first edition of *The Principall Navigations, Voiages, Traffiques and Discoueries of the English Nation* followed in 1589, expanded into three volumes in 1598–1600. The latter, a truly voluminous compilation of English and broader European travel narratives (a modern edition spans 11 volumes and over 5,000 pages in total, with another volume devoted to indexes) is the focus of present investigation, examining the representation of colonial violence through computational text analysis and testing the digital methods against a concrete research project.\n",
    "\n",
    "Despite the obvious challenges posed by the shaky standardization and odd spellings of Early Modern English, Hakluyt is paritcularly well-suited for computational methods due to the sheer volume of text; particular affordances are further opened up by his relative discursive consistency, as will be expanded below. Digital Humanities (which I roughly lump here with computational text analysis or text mining) have been hailed as redeeming litearary studies from the limits of what is humanly readable--tens, at most hundreds of works--imposing a narrow canon out of the tens and hundreds of thousands of works published since the advent of print (and perhaps implicitly delivering the discipline from the low prestige of subjective Humanities-style scholarship). It has also been derided as at best a gimmick and at worst a blatant neoliberalist takeover of one of the last bastions of critique. In the current project, however, the limitations of text mining, and in particular the simplistic and 'flat' nature of the query results, will be not only acknowledged by exploited as part of the inverstigation. Tracking the representations of violence in Hakluyt's collection, I will use digital methods, particularly word frequency counts and topic modeling, as a proxy for a hasty or naive reading, and then contrast it with closer reading of sample passages. I argue that Hakluyt dramatically downplays colonial violence, and is in fact much more concerned with intra-European conflict (and primarily with hostilities between English and Spanish), while colonial violence is often not explicitly registered as violence, and takes more sophisticated methods to uncover. The vast volume of the text, conducive to digital methods, precludes a full or even a representative close reading, and so I will not aspire to definitively solve this problem, but rather raise valuable questions.\n",
    "\n",
    "*The Principal Navigations*, famously termed \"the Prose Epic of the modern English nation\" (James A. Froude, 1852), reads vehemently nationalist in its commitment to British expansion and power as well as in its principled stand against the Spanish as a curb on said English power (though some of the modern connotations of nationalism are no doubt an anarchronism for any 16th-century writer). Hakluyt could thus be expected to play up the Black Spanish Legend at every opportunity with a Las Casas-like litany of colonial atrocities to both denigrate the Spanish and call for a benevolent British alternative (as he indeed has done in the *Discourse Concerning Western Planting*). Overt instances of violence in the reports, however, address intra-European conflict at least as often as they do properly colonial violence (meaning European-American conflict). Given the scope of 16th-century colonial devastations, an equal representation is indeed a dramatic misrepresentation somewhat evocative of Rob Nixon's 1994 argument about \"slow violence.\" Nixon investigates the disparity in media and activism attention between bounded instances of spectacular violence featuring clearly identifiable acts and perpetrators (9/11 comes to mind as a particularly stiking example) and the objectively vaster human harms of enviromental degradation, which often slip by unnoticed by the centers of power. Although colonial violence was often 'spectacular' in the most horrific of ways, it clearly gets the short shrift in *The Principal Navigations*; here as well as in Nixon's research, the victims who suffer a second time through erasure are the oppressed and disempowered. The suffering of the American 'Indians,' like that of today's poor (often also located in the Global South), becomes second-rate, implying that they are effectively counted less than human, worth less than English or even enemy Spanish. The symptomatic hasty or naive computer-reading that skims over American suffering will be complemented by spot-readings on ostensibly low-violence passages giving an occasional taste of the kinds of oppression that fail to register as violence, many of which cluster around issues of property or possession and the evocative keyword 'take.'\n",
    "\n",
    "The project derives from an open-ended exploration of *The Principal Navigations* as much concerned with training myself in computational text analysis as with producing research findings as such, and it retains some of the mesiness of that process. The current notebook essentially shapes code and analysis into a coherent narrative while trying to clean up the programming. Where the analysis runs internally in Python, I will make sure that it runs right, but where external tools are used--epsecially regarding topic modeling, where every run is necessarily different due to the random component of the algorithm--I will rely on the products of the original computations.\n",
    "\n",
    "The notebook begins with text cleaning, which, though distinctly unexciting, is a necessary foundation for any computational analysis. It proceeds with some basic numbers on the scope and simple characteristics of *The Principal Navigations*, and then surveys the broad discursive qualities of the text through word frequencies and topic modeling. Finally, I zero in on matters of violence, first exploring its discursive neighborhood through word frequencies and network graphs, and then diving into more specific detailed analysis with topic modeling. The main finding will be the approximate ratio between representations of intra-European violence and representations of colonial violence; the under-representation of colonial violence will be given further dimension through a series of close readings that point out the violence that fails to register through the computer reading. In this way, the very limitations of computational text analysis reinforce the argument: in only registering the most obvious instances of violence, the algorithm shows us a valuable lesson on silencing and erasure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc48f92-10dc-4ee6-9dc8-662afeda3162",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db15512f-554d-4973-8c45-c1b6201f94a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Text cleaning is widely acknowledged to be in equal measures crucial and unexciting; feel free to search-hop to the \"Basic Numbers Rundown\" section down below unless you have a practical interest in the process (perhaps particularly in handling Early Modern English spelling challenges)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61688d77-20d8-425d-aa65-b821bd09950c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Selection\n",
    "\n",
    "Oxford University Press has contracted a critical edition of *The Principal Navigations*, but its website at http://www.hakluyt.org/ is currently down and it is in any case still a work in progress ([cf. website here](https://mooreinstitute.ie/projects/the-hakluyt-edition/)). A [report from that project](https://ora.ox.ac.uk/objects/uuid:9f4e7aa8-7368-4085-bb28-ce45e24e3a19) indicates a number of available editions, \"including H.R Evans’s five-volume *Hakluyt’s Collection of the Early Voyages, Travels, and Discoveries of the English Nation* (1809‒12); a ‘rearranged’ version by Edmund Goldsmid (published in Edinburgh in sixteen volumes, 1885‒90)); the twelve-volume edition prepared by James MacLehose and Sons in Glasgow (1903‒05); and the Dent Everyman edition in eight volumes (1907) which excluded Hakluyt’s Latin texts.\" Like the project editors, I have found Goldsmid's rearrangements and Dent's modifications to compromise the integrity of the source. The project ultimately chose the TCP transcription of the original edition [available through EEBO](https://quod.lib.umich.edu/e/eebo/A02495.0001.001?view=toc) as their starting point, but the advantage of the proximity to the original text is negated for my purpose by its strict adherence to Early Modern English spelling conventions, paritcularly the frequent substitutions of u for v and vice-versa. The MacLehose edition conveniently modernizes that particular usage, and I chose to use that instead. I was able to access a PDF version through [Cambridge Core](https://www.cambridge.org/core/search?q=%22The+Principal+Navigations+Voyages+Traffiques+and+Discoveries+of+the+English+Nation%22&_csrf=Yf0FTrIr-KAzLwYWrAIZ6B5I2_7Pr1rlHFl8). In retrospect, the Goldsmid edition, freely available [here](http://onlinebooks.library.upenn.edu/webbin/metabook?id=hakluyt), would have sufficed and even proven advantageous. When I ran my core analysis on the third, American volume of the *Navigations*, I had to selectively exclude reports concenrning travels through the Pacific and on towards China--while Goldsmid conveniently \"grouped together those voyages which relate to the same parts of the globe, instead of adopting the somewhat haphazard arrangement of the original edition.\"\n",
    "\n",
    "## Text Extraction from PDFs (with basic cleaning)\n",
    "\n",
    "Cambridge Core offers clean scans in PDF format with an underlying text layer (thus, selectable & searchable rather than merely a static image scan), one file per chapter or section in the paper volume. Each volume follows a standard numbered file naming format, so I downloaded each into a sepaarate folder to avoid confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c8285f9-0e05-4a1a-97b9-4572c40eef58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"300\"\n",
       "            src=\"text-data/sample.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x29f42d93520>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"text-data/sample.pdf\", width=600, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd87fb-064f-4f2b-aeef-007273768b97",
   "metadata": {
    "tags": []
   },
   "source": [
    "As seen in the sample PDF, there are a few obvious challenges: headers and footers, sidenotes, intrusions from the preceding and next chapters, and an ornamental letter at the opening of each chapter that evades OCR. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557b998-e941-4d5a-8668-5a5e6a76a97d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PDFminer attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a86575-670c-4b72-ab84-7fa6e5171896",
   "metadata": {
    "tags": []
   },
   "source": [
    "I have first tried to use the PDF text layer as given through the [pdfminer library](https://pypi.org/project/pdfminer/), leaving out blank lines, blank pages, and headers & footers. I also used the process to extract the relelvant date from the headers, encoding the following metadata in the text filename: volume number, chapter number, geographical region, date, title, and page range. The code ran as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eecacc93-f7e7-48e3-a69d-1d083deb8856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.high_level import extract_pages\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59843e3-3065-4af0-ad6d-fc2b4817cb19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_blank_lines(text):\n",
    "    '''\n",
    "    remove blank lines from text given as list of lines\n",
    "    parameters: text: chapter text split into lines\n",
    "    returns: all non-empty lines from text\n",
    "    '''\n",
    "    prev_len = len(text) + 1\n",
    "    while len(text) < prev_len:\n",
    "        prev_len = len(text)\n",
    "        try:\n",
    "            text.remove('')\n",
    "        except: pass\n",
    "    return(text)\n",
    "\n",
    "def find_date(headers):\n",
    "    '''\n",
    "    extract date from list of chapter headers\n",
    "    parameters: headers: list of lines from chapter headers\n",
    "    returns: date identified as the most common numberical component of the lines text zeroed to 4 digits\n",
    "    '''\n",
    "    headers_string = ''.join(headers)\n",
    "    numbers = re.findall(r'[0-9]+', headers_string)\n",
    "    if len(numbers) == 0: return 'XXXX'\n",
    "    elif len(numbers) == 1: common_number = numbers[0]\n",
    "    else:\n",
    "        common_number = max(set(numbers), key = numbers.count)\n",
    "    if int(common_number) > 300 and int(common_number) < 1620:\n",
    "        return common_number.zfill(4)\n",
    "    else:\n",
    "        return 'XXXX'\n",
    "\n",
    "def empty_page(lines):\n",
    "    '''\n",
    "    determine whether a page is empty of text content\n",
    "    parameters: lines: page text as lines\n",
    "    returns: True  if page contains less than 10 lines or lines average less than 3 chars\n",
    "    '''\n",
    "    #if len(lines_sans_blanks) < 10: return True  /// original mis-variable\n",
    "    if len(lines) < 10: return True\n",
    "    lines_lens = [len(line) for line in lines]\n",
    "    if sum(lines_lens)/len(lines_lens) < 4: return True    \n",
    "    return False\n",
    "\n",
    "def chapter_process(chapter):\n",
    "    '''\n",
    "    extract chapter text free of headers & footers and determine date\n",
    "    parameters: chapter: path of pdf chapter\n",
    "    returns: \n",
    "        chapter text as single string \n",
    "        date with leading zeroes to 4 digits or XXXX if failed to extract\n",
    "    possible enhancements:\n",
    "        resolve linebreak dashes\n",
    "        crop out side notes\n",
    "    '''\n",
    "    headers = []\n",
    "    chapter_text_list = []\n",
    "    chapter_pages = len(list(extract_pages(chapter)))\n",
    "\n",
    "    for pagenum in range(chapter_pages): \n",
    "        # process page by page, clearing headers, footers & blank lines; storing headers for date extract\n",
    "        text = extract_text(chapter, page_numbers = [pagenum])\n",
    "        lines = text.splitlines()\n",
    "        lines_sans_blanks = remove_blank_lines(lines)\n",
    "        if empty_page(lines_sans_blanks): continue\n",
    "        headers += lines_sans_blanks[:3]\n",
    "        if not ('.1_pp' in chapter and pagenum == 0):  #remove header except for first page of first chapter in each volume, which has no header\n",
    "            del lines_sans_blanks[:3]\n",
    "        del lines_sans_blanks[-4:] # remove footer\n",
    "        chapter_text_list += lines_sans_blanks\n",
    "\n",
    "    #override hyphen-broken words at ends of lines excepting the last\n",
    "    for i in range(len(chapter_text_list) - 1):\n",
    "        if chapter_text_list[i][-1] == '-':\n",
    "            chapter_text_list[i] = chapter_text_list[i][:-1] + chapter_text_list[i+1]\n",
    "            chapter_text_list[i+1] = ' '\n",
    "\n",
    "    #joining list of lines into one string and cleaning out extra spaces\n",
    "    chapter_text_string = ' '.join(chapter_text_list)\n",
    "    chapter_text_string = re.sub('\\s+',' ', chapter_text_string)\n",
    "    return(chapter_text_string, find_date(headers))\n",
    "\n",
    "def vol_chap_geog_prange(vol, chapter):\n",
    "    '''\n",
    "    extract volume, chapter number, broad geographical designation and title ready for feeding into txt file names\n",
    "    parameters: vol as int; chapter as path of pdf chapter\n",
    "    returns: \n",
    "            vol_z as number zeroed to 2 digits\n",
    "            chap_z as number zeroed to 2 digits\n",
    "            geog as CCCC determined by volume / chapter numbers below:\n",
    "                01.01-04.4: NNE-\n",
    "                04.05-06.17: SSE1\n",
    "                06.18-07.16: SSE2\n",
    "                07.17-11.43: AM-- \n",
    "            title as extracted from file name\n",
    "            page range zeroed to 3 digits each number\n",
    "    '''\n",
    "    chap = chapter[chapter.find('.') + 1 : chapter.find('_')] #extract chap num between first dot and first underscore\n",
    "    geog = 'XXXX' #to raise flag just in case something escapes\n",
    "    if vol in [1,2,3]:\n",
    "        geog = 'NNE-'\n",
    "    elif vol == 4:\n",
    "        if int(chap) in range(5): geog = 'NNE-'\n",
    "        else: geog = 'SSE1'\n",
    "    elif vol == 5:\n",
    "        geog = 'SSE1'\n",
    "    elif vol == 6:\n",
    "        if int(chap) in range(18): geog = 'SSE1'\n",
    "        else: geog = 'SSE2'\n",
    "    elif vol == 7:\n",
    "        if int(chap) in range(17): geog = 'SSE2'\n",
    "        else: geog = 'AMER'\n",
    "    else: geog = 'AMER'\n",
    "\n",
    "    title = chapter[:-4]\n",
    "    for i in range(4): # remove section & pages through 4th underscore\n",
    "        title = title[title.find('_') + 1:]\n",
    "\n",
    "    page_range = chapter\n",
    "    for i in range(2):\n",
    "        page_range = page_range[page_range.find('_') + 1:]\n",
    "    page_start = page_range[:page_range.find('_')]\n",
    "    page_range = page_range[page_range.find('_') + 1:]\n",
    "    page_end = page_range[:page_range.find('_')]\n",
    "    page_range = page_start.zfill(3) + '-' + page_end.zfill(3)\n",
    "    return(str(vol).zfill(2), chap.zfill(2), geog, title, page_range)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa7344-361d-46ec-a5a4-f163bf42e2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate over volumes, then over chapter pdfs excluding front matter etc\n",
    "for vol in range(1,12):\n",
    "    filelist = os.scandir(os.getcwd() + '/' + str(vol))\n",
    "    for entry in filelist:\n",
    "        if entry.is_file(): \n",
    "            if (((vol == 1 and entry.name.startswith('06')) \n",
    "                or (vol > 1 and entry.name.startswith('04'))) \n",
    "                and entry.name[3] != '0'):  #identify body chapters\n",
    "                    print(str(vol) + '_' + entry.name)\n",
    "                    chapter = str(vol) + '/' + entry.name\n",
    "                    \n",
    "                    #extract and save text, extract file name components and fit into file name\n",
    "                    chapter_text, date = chapter_process(chapter)\n",
    "                    vol_n, chap_n, geog, title, page_range = vol_chap_geog_prange(vol, chapter)\n",
    "                    filename = vol_n + '_' + chap_n + '_' + geog + '_' + date + '_' + title + '_pp.' + page_range\n",
    "\n",
    "                    #create text file\n",
    "                    with open(filename + '.txt', 'w') as f:\n",
    "                        f.write(chapter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5e60d-89e1-46c7-960f-616e9e17f77b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The process worked reasonably well, but the original OCR largely treated the sidenotes as an extension of the text, substantially garbling the original sentences whenever a sidenote showed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977cd41-b6f1-4eae-a609-7e29b91d6097",
   "metadata": {
    "tags": []
   },
   "source": [
    "### FineReader extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26633442-1ebb-4403-b2e2-a8bf0cc621f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "[ABBYY FineReader](https://pdf.abbyy.com/), applied to the PDFs, performed better, and I re-OCR'd the 589 files using the batch function, resulting in text files where the side notes, when correctly identified as such (definitely not always, but perhaps half the time), formed distinct paragraphs. The original PDF volumes were kept in separate folders, FineReader reproduced that structure, so my processing had to account for that as I wanted to have all files in a single folder.\n",
    "\n",
    "Having already formed the filenames as I wanted them, I created a ledger file for easy retrieval and reference; a colleague has suggested I keep the metadata exclusively in a ledger and trim the file names down to a minimum, but I chose the immediate availability of filename-metadata over the efficiency and elegance of referencing it through a ledger every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d0c52a-583a-4061-8ec1-5cd453ff08ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def HK_date(fileid):\n",
    "    #extracts date component from Hakluyt text file name, returns as integer\n",
    "    return(int(fileid[11:15]))\n",
    "def HK_geog(fileid):\n",
    "    #extracts geography component from Hakluyt text file name, returns as string\n",
    "    return(fileid[6:10])\n",
    "def HK_title(fileid):\n",
    "    #extracts geography component from Hakluyt text file name, returns as string\n",
    "    return(fileid[16:-15])\n",
    "def HK_vol(fileid):\n",
    "    #extracts volume component from Hakluyt text file name, returns as integer\n",
    "    return(int(fileid[:2]))\n",
    "def HK_chap(fileid):\n",
    "    #extracts chapter component from Hakluyt text file name, returns as integer\n",
    "    return(int(fileid[3:5]))\n",
    "def HK_pages(fileid):\n",
    "    #extracts page range component from Hakluyt text file name, returns as tuple of integers\n",
    "    pages_char = fileid[-11:-4]\n",
    "    pages_char_split = pages_char.split('-')\n",
    "    return(int(pages_char_split[0]), int(pages_char_split[1]))\n",
    "def HK_page_length(fileid):\n",
    "    #extracts page length component from Hakluyt text file name, returns as integer\n",
    "    first_page, last_page = HK_pages(fileid)\n",
    "    return(last_page-first_page+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18ddf806-6787-4e96-bf6a-1764bc8bc473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('text-data/ledger.csv', 'w', newline='') as ledgerfile: \n",
    "    ledger = csv.writer(ledgerfile) \n",
    "    ledger.writerow(['vol', 'chap', 'geog', 'date', 'title', 'pages'])\n",
    "    filelist = os.scandir('text-data/CambridgeCore MacLehose pdfminer extract')\n",
    "    for file in filelist:\n",
    "        ledger.writerow([HK_vol(file.name), HK_chap(file.name), HK_geog(file.name), HK_date(file.name), HK_title(file.name), str(HK_pages(file.name)[0]).zfill(3)+'-'+str(HK_pages(file.name)[1]).zfill(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc84459-4e5c-4564-a26d-c747f291ce36",
   "metadata": {
    "tags": []
   },
   "source": [
    "Thus, processing the FineReader output, I merely did what I could to clean out headers & footers, and drew the formatted file names from the ledger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d30042d-d7ee-4829-a28c-e856138de8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text(path):\n",
    "    '''\n",
    "    clean up a text file from FineReader\n",
    "    parameters: path: text file path\n",
    "    returns: text as string cleaned of headers & footers based on Cambridge Hakluyt edition\n",
    "    '''\n",
    "    clean_lines = []\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.readlines()\n",
    "        i = 0\n",
    "        while i < len(text) - 1:\n",
    "            #skip up to 4 short lines - suspects for header remnants\n",
    "            if len(text[i]) < 35: \n",
    "                i += 1\n",
    "                if i > len(text) - 1: break\n",
    "            if len(text[i]) < 35: \n",
    "                i += 1\n",
    "                if i > len(text) - 1: break\n",
    "            if len(text[i]) < 35: \n",
    "                i += 1\n",
    "                if i > len(text) - 1: break\n",
    "            if len(text[i]) < 35: \n",
    "                i += 1\n",
    "                if i > len(text) - 1: break\n",
    "            # after header, collect body text until footer start\n",
    "            while not 'https://' in text[i] or 'The material originally positioned' in text[i]:\n",
    "                clean_lines.append(text[i])\n",
    "                i+=1\n",
    "                if i >= len(text) - 1: break\n",
    "            #once footer starts, skip footer lines, check for page number and repeat loop\n",
    "            while 'https://' in text[i] or 'The material originally positioned' in text[i]:\n",
    "                i+=1\n",
    "                if i > len(text) - 1: break\n",
    "            if i > len(text) - 1: break\n",
    "            #check for page number that occasionally precedes footer -- now in clean text\n",
    "            if len(clean_lines[len(clean_lines) - 1]) < 4:\n",
    "                clean_lines.pop()\n",
    "    #returns text as string with newlines preserving line braks just in case\n",
    "    return('\\n'.join(clean_lines))\n",
    "def create_filename(vol, file_name):\n",
    "    '''\n",
    "    create file name based on format set in pdfminer extract, including date, drawing on CSV ledger\n",
    "    parameters:\n",
    "        vol: number of volume as integer\n",
    "        file_name: text file name\n",
    "        returns: new filename as string\n",
    "    csv headers: vol chap geog date title pages\n",
    "    '''\n",
    "    #format vol to match ledger entries\n",
    "    vol = str(vol).zfill(2)\n",
    "    #slice chapter portion of filename\n",
    "    chap = file_name[3:5]\n",
    "    #correct for chaps 0 through 9\n",
    "    if chap[1] == '_':\n",
    "        chap = '0' + chap[0]\n",
    "    with open ('text-data/ledger.csv', mode = 'r', newline = '') as ledger:\n",
    "        ledger_reader = csv.DictReader(ledger)\n",
    "        #iterate over csv rows until find relevant one\n",
    "        for row in ledger_reader:\n",
    "            if row['vol'] == vol and row['chap'] == chap:\n",
    "                r = row\n",
    "                break\n",
    "    name = '_'.join([vol, chap, r['geog'], r['date'], r['title']])\n",
    "    return(name + '_pp.' + r['pages'] + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a50e0648-2468-4a34-be58-c93503b0ec0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#iterate through volumes\n",
    "os.mkdir('text-data/ARG/')\n",
    "for vol in range(1,12):\n",
    "    #filelist = os.scandir('text-data/Cambridge MacLehose FineReader OCR/' + str(vol))\n",
    "    #having processed the FineReader output once, I removed the original files to an archive folder; I have to account for that here\n",
    "    filelist = os.scandir('text-data/archive/Cambridge MacLehose FineReader OCR raw/' + str(vol))\n",
    "    for entry in filelist:\n",
    "        #identify body chapters\n",
    "        if entry.is_file(): \n",
    "            #only pick up body chapters rather than front matter and such\n",
    "            if (((vol == 1 and entry.name.startswith('06')) \n",
    "                or (vol > 1 and entry.name.startswith('04'))) \n",
    "                and entry.name[3] != '0'):\n",
    "                # get clean text and desired filename\n",
    "                text = process_text(entry.path)\n",
    "                filename = create_filename(vol, entry.name)\n",
    "                # create text file\n",
    "                with open('text-data/ARG/'+filename, 'w', encoding=\"utf8\") as f:\n",
    "                    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd7e2e-6acc-4f4b-aba0-54a782b4400e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73c4fd-190a-4a7a-ae47-579d57074149",
   "metadata": {
    "tags": []
   },
   "source": [
    "While there is some basic text cleaning in the above code, I had on multiple later occasions realized that I could or needed to do better, further branching and splitting up my text corpuses. I aggregate the text cleaning operations below for the sake of organization, but it is likely that I will keep having to scrub mid-way through later research projects too as issues or opportunities come up unexpectedly.\n",
    "\n",
    "Thus, recognizing that some headers and artifacts that probably mark volume and page number of the original edition (e.g., \"[II. i. 281.]\") made it into the text, I ran an extra scrub as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fff9692d-dded-4ded-b448-7ef44b97471d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filelist = os.scandir('text-data/ARG/')\n",
    "for entry in filelist:\n",
    "    with open(entry.path, 'r', encoding=\"utf8\") as fr:\n",
    "        text = fr.read()\n",
    "        text = text.replace('THE ENGLISH VOYAGES', '')\n",
    "        text = re.sub(r'\\[.{1,9}?\\]', '', text)\n",
    "        with open('text-data/ARG/' + entry.name, 'w', encoding=\"utf8\") as fw:\n",
    "            fw.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e51e8-c6cb-4986-915e-b33b673da003",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "**Note: potentially far more elegant computer-vision solution to headers, footers, and sidenotes**\n",
    "\n",
    "William Mattingly has put forward a far more refined solution for trimming excess text on a page [here](https://github.com/wjbmattingly/text-analysis-for-ancient-and-medieval-languages/blob/main/ancient_medieval_02.ipynb), but I've never gotten around to getting the basic facility with OpenCV necessary to adapt his code to my case; in retrospect, I would not have gained much by this improvement, though it still irks me now and then.\n",
    "***\n",
    "\n",
    "An important data stewardship issue comes up with operations such as text cleaning: do I just overwrite the 'dirty' corpus, potentially losing data, or do I create a new copy, leaving behind a trail of useless data? I opted for the second approach in my original workflow--which made a lot of sense especially as I was testing things out and could easily, say, overwrite the corpus with empty spaces or, in some ways worse, get what looks like the desired result but lose or grarble 30% of the data. As I am working with verified code in this cleaned-up notebook, I am opting to overwrite whenever it makes sense, but it is a choice worth addressing explicitly. \n",
    "\n",
    "In another attempt to patch up an imperfect text corpus, I tried standardizing variant spellings by simple find and replace. By the time I got to it, I had multiple corpuses, which complicated everything (see the 'text cleaning' notebook); had I thought of it earlier in the process, it could have been much simpler, as below. As I only got to it late in the process, I did not use it extensively except to consolidate spellings of tokens I had particular interest in, such as violence indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c8f51bea-25c9-4480-aeff-34a7465a3053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace(old, new, folder):\n",
    "    '''\n",
    "    replaces all instances of 'old' token with 'new' token across corpuses defined through 'scope'; \n",
    "    records replacement in text file\n",
    "    arguments:\n",
    "        old (str) : token to be replaced\n",
    "        new (str) : substitute token \n",
    "        folder: folder within text-data to \n",
    "        '''\n",
    "    filelist = os.scandir('text-data/' + folder)\n",
    "    for entry in filelist:\n",
    "        with open(entry.path, 'r', encoding=\"utf8\") as fr:\n",
    "                text = fr.read()\n",
    "                # replacement through regex to account for adjacent punctuation & ignore case\n",
    "                # regex fails if 'new' starts with a number; prefacing the number with a space gets around that\n",
    "                text = re.sub(r'([^a-zA-Z]|^)('+ old + r')([^a-zA-Z])',r'\\1' + new +r'\\3', text, flags=re.I)\n",
    "        with open(entry.path, 'w', encoding=\"utf8\") as fw:\n",
    "                fw.write(text)\n",
    "    with open ('text-data/'+ 'ARG_replacement_record.txt', 'a', encoding=\"utf8\") as f:\n",
    "        f.write(old + ' -> ' + new + ' in ' + folder + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "50f6920f-4bd7-47d7-9e49-3a66fc34aaac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sw = pd.read_csv('text-data/stopwords.csv')\n",
    "stop_words = [w for w in (sw['nltk'].tolist() + sw['eliz'].tolist() + sw['hk'].tolist() + sw['pronouns'].tolist()) if pd.isnull(w) == False]\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'text-data/ARG'\n",
    "hakluyt = PlaintextCorpusReader(corpus_root, '.*')\n",
    "hakluyt_col = nltk.TextCollection(hakluyt)\n",
    "hakluyt_fd = nltk.FreqDist(word for word in hakluyt.words() if word.isalpha() and word not in stop_words)\n",
    "#nltk collection & freqdist for identifying candidates for spelling scrub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02d8967e-03e7-419a-a843-a7b96299e001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I|The|great|one|And|men|day|came|two|made|good|time|called|many|land|place|come|said|king|people|ships|certaine|found|sea|leagues|river|man|three|water|much|went|sayd|part|countrey|ship|sent|first|make|every|set|towne|In|things|England|God|night|dayes|way|brought|English|A|small|side|tooke|goe|coast|But|John|Captaine|This|foure|company|take|S|yeere|voyage|long|Island|They|see|put|taken|West|divers|downe|For|owne|Cape|North|shore|M|course|degrees|rest|thence|South|places|neere|departed|five|away|Master|goods|together|maner|est|comming|hundred|winde|done|halfe|store|others|thing|Indians|give|farre|use|returne|East|towards|thought|say|high|number|From|letters|reason|morning|Lord|left|little|Generali|THE|cause|name|Spaniards|betweene|order|sixe|quod|gave|Hand|shot|last|must|shippe|meanes|saw|citie|William|faire|victuals|house|ENGLISH|VOYAGES|houses|Thomas|shippes|aboord|"
     ]
    }
   ],
   "source": [
    "# print out high-frequency words to identify the most frequent misspellings to maximize impact of spelling scrub\n",
    "for token, _ in hakluyt_fd.most_common(150):\n",
    "     print(token, end = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9b6be553-de84-48a0-8d1a-faefbfa9516a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hard', 'shalbe', 'chamber', 'habere', 'haled', 'hailed', 'handled', 'altered', 'hale', 'hald', 'healed', 'haberet', 'habetur', 'halsers', 'habited', 'halberds', 'Chamber', 'halters', 'haberi', 'halser', 'halberd', 'haberem', 'Shalbe', 'herd', 'Calaber', 'Chaleur', 'haulser', 'halbard', 'halbards', 'chalybe', 'haerede', 'Kaleber', 'habenda', 'shaled', 'alberta', 'haleberts', 'Thaber', 'halberdiers', 'haeredi', 'haberes', 'halbardes', 'habendi', 'halowed', 'Halberds', 'Filberd', 'halter', 'halted', 'hayled', 'shoaled', 'haberdash', "
     ]
    }
   ],
   "source": [
    "# use fuzzy search to identify misspellings of terms of especial interest\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process \n",
    "for word in hakluyt_col.vocab():\n",
    "    if fuzz.ratio('halberd', word) > 70:\n",
    "        print(\"'\"+word+\"',\", end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "16cea5d3-783b-4b4d-8f01-93a7f2dd539b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "#concordance to make sure that I am indeed dealing with a misspelling and not a separate term\n",
    "hakluyt_col.concordance('naturali', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b7dbfbb8-7434-4ceb-ae48-b46323c393d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for w in ['halbert', 'halebert']:\n",
    "    replace(w,'halberd', 'ARG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be3ad9-9349-4cac-a368-ac932a0743c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trimming Out Duplicate Passages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca80c03-8cb7-4695-8cf4-77143b37c57c",
   "metadata": {
    "tags": []
   },
   "source": [
    "A more substantial clean-up operation concerned the little portions of previous and next chapters in each PDF file (excepting the rare cases where a chapter starts or ends on a clean page-break)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7baa40a6-11d8-4e21-90e6-8983f92aff89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259  chapters out of 589 are 3 pages or shorter and thus substantially impacted by imprecise chapter divisions based on pages rather than chapter title location\n"
     ]
    }
   ],
   "source": [
    "#creating page length frequency distribution to identify number of chapters 3 page long or less\n",
    "hakluyt_lengths = [HK_page_length(fileid) for fileid in hakluyt.fileids()]\n",
    "length_fd = nltk.FreqDist(hakluyt_lengths)\n",
    "print(length_fd[1]+length_fd[2]+length_fd[3],' chapters out of 589 are 3 pages or shorter and thus substantially impacted by imprecise chapter divisions based on pages rather than chapter title location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faca124-089d-4e20-b0a8-f4c5d5b3f222",
   "metadata": {
    "tags": []
   },
   "source": [
    "Given that a full PDF page is reproduced at each chapter split, there is no algorithmic way to decide on where the split should happen (unless I retool the OCR to capture text size to be able to identify the title, perhaps outputting html instead of raw text as I had). I ended up using the titles that came with the PDF files--which, interestingly, match up to the table-of-content titles, which are similar to but not identical to the titles that appear in the text. I did not, unfortunately, think to look up fuzzy matching at the time, so instead I made my own crude version of that. I was really reluctant to lose data when my matching failed, so I opted for a time-intensive (several days of work) manual confirmation of the break line as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1653668-40f0-4705-898e-0d91e9157740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# core functions:\n",
    "# - title_match: ratio of stopword-filtered words from ledger title matched in the section\n",
    "# - page-approx: flags true once string equals/surpasses maximum text likely to fit on a single page\n",
    "# - user-select: print out page sections with numbers and mark user input\n",
    "\n",
    "# algo:\n",
    "# 1. split chapter into line sections\n",
    "# 2. assemble first_page\n",
    "#     1. identify highest title_match\n",
    "#     2. print out leger title, max title_match, collect user input\n",
    "#     3. if input == y, store title_match position\n",
    "#     4. else user-select position\n",
    "# 3. assemble last_page\n",
    "#     1. identify highest title_match\n",
    "#     2. print out leger title, max title_match, collect user input\n",
    "#     3. if input == y, store title_match position **from end**\n",
    "#     4. else user-select position **from end**\n",
    "# 4. new file\n",
    "#     1. assemble from total lines based on start and end indexes\n",
    "#     2. save into new folder\n",
    "def title_match(title, section):\n",
    "    \"\"\"determines ratio of stopword-filtered words from ledger title matched in the section\"\"\"\n",
    "    section_trim = section[:int(len(title)*2)]\n",
    "    title_filtered = [word for word in title if word not in stop_words]\n",
    "    matching_words = [word for word in title_filtered if word in section_trim]\n",
    "    return len(matching_words) / len(title_filtered)\n",
    "def page_approx(lines):\n",
    "    '''flags true once lines equal/surpass maximum text likely to fit on a single page'''\n",
    "    return True if len(' '.join(lines)) >= 2600 else False\n",
    "def user_select(title, lines):\n",
    "    '''prints title & page sections w/index and returns user input on decided match'''\n",
    "    print('_____________________________________________________________________________')\n",
    "    print('select best match for: ', title)\n",
    "    print('-----------------------------------------------------------------------------')\n",
    "    for line in lines:\n",
    "        if len(line) > 3: \n",
    "            print(lines.index(line), ': ', line[:400])\n",
    "    print('_____________________________________________________________________________')\n",
    "    print('select best match for: ', title)\n",
    "    print('-----------------------------------------------------------------------------')\n",
    "    return(int(input('select line')))\n",
    "def next_title(raw_title):\n",
    "    '''based on current title, accesses ledger to determine the next chapter title\n",
    "    return next chapter title + next chapter starting page for overlap calc'''\n",
    "    with open('text-data/ledgertagged.csv') as ledgertaggedcsv:\n",
    "        ledger = list(csv.DictReader(ledgertaggedcsv, delimiter = \",\"))\n",
    "        for row in ledger:\n",
    "            if int(row['vol'])==HK.HK_vol(raw_title) and int(row['chap'])==HK.HK_chap(raw_title):\n",
    "                if int(row['vol']) == 11 and int(row['chap'])== 43: #checking for last entry\n",
    "                    return ''\n",
    "                else:\n",
    "                    next_title = ledger[ledger.index(row) +1]['title']\n",
    "                    next_chapter_start_page = int(ledger[ledger.index(row) +1]['pages'][:3])\n",
    "                    return(\" \".join(next_title.split('_')),next_chapter_start_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d21f5a-be0d-4a62-ac4b-72d1103f67cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filelist = os.scandir('text-data/ARG')\n",
    "newdir = r'text-data/ARG/'\n",
    "end_title = '' #initializing flag for known title based on end_title from preceding chapter\n",
    "char_block = '''||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
    "                ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||'''\n",
    "for entry in filelist:\n",
    "    #skip past completed files\n",
    "    last_vol_n, last_chap_n = 11, 28\n",
    "    if HK.HK_vol(entry.name) < last_vol_n or (HK.HK_vol(entry.name) == last_vol_n and HK.HK_chap(entry.name) <= last_chap_n):\n",
    "        continue\n",
    "    #determine start line\n",
    "    with open(entry.path, 'r', encoding=\"utf8\") as f:\n",
    "        chapter_lines = f.readlines()\n",
    "        chapter_lines.append(' ') #appending extra line in case I need to end the chapter after the last line when page brak matches chapter break\n",
    "        chapter_title = \" \".join(HK.HK_title(entry.name).split('_'))\n",
    "        first_page = []\n",
    "        line_index = 0\n",
    "        while not (page_approx(first_page) or line_index == len(chapter_lines)):\n",
    "            first_page.append(chapter_lines[line_index])\n",
    "            line_index += 1\n",
    "        if end_title == '':\n",
    "            print(char_block*2)\n",
    "            print(entry.name)\n",
    "            print('find start line')\n",
    "            best_match = 0\n",
    "            for line in first_page:\n",
    "                if title_match(chapter_title, line) > best_match:\n",
    "                    best_title = line\n",
    "                    best_match = title_match(chapter_title, line)\n",
    "            title_approved = input(f'--target: {chapter_title}, \\n--auto match: {first_page.index(best_title)} : {best_title[:400]} \\n--approve? y/n')\n",
    "            if title_approved == 'y':\n",
    "                start_index = first_page.index(best_title)\n",
    "            else:\n",
    "                start_index = user_select(chapter_title, first_page)\n",
    "        else:\n",
    "            start_index = first_page.index(end_title)\n",
    "    #determine last line\n",
    "        print(char_block)\n",
    "        print(entry.name)\n",
    "        print('find end line')\n",
    "        chapter_title, next_chap_start_page = next_title(entry.name)\n",
    "        #flag if next chapter starts on new page\n",
    "        _, chapter_end_page = HK.HK_pages(entry.name)\n",
    "        if next_chap_start_page == chapter_end_page + 1:\n",
    "            print('||| next chapter starts on new page |||')\n",
    "        last_page = []\n",
    "        #print('last page just after creation', last_page)\n",
    "        line_index = -1\n",
    "        while not (page_approx(last_page) or line_index == len(chapter_lines)*-1):\n",
    "            last_page.append(chapter_lines[line_index])\n",
    "            line_index -= 1\n",
    "        #print('last page initially assembled', last_page)\n",
    "        last_page.reverse()\n",
    "        #print('last page after reversal', last_page)\n",
    "        #print('last page length', len(last_page))\n",
    "        best_match = 0\n",
    "        for line in last_page:\n",
    "            if title_match(chapter_title, line) > best_match:\n",
    "                best_title = line\n",
    "                best_match = title_match(chapter_title, line)\n",
    "        title_approved = input(f'--target: {chapter_title}, \\n--auto match: {best_title[:400]} \\n--approve? y/n')\n",
    "        if title_approved == 'y':\n",
    "            end_index = last_page.index(best_title)\n",
    "        else:\n",
    "            end_index = user_select(chapter_title, last_page)\n",
    "        #print('end index first established', end_index)\n",
    "        end_index = end_index - len(last_page) # flipping to count from end\n",
    "        #print('end index flipped', end_index)\n",
    "    #write new file\n",
    "    new_lines = chapter_lines[start_index : end_index]\n",
    "    #no need for +1 in end_index as I don't want to include next title in the text of the chapter\n",
    "    new_text = '\\n'.join(new_lines)\n",
    "    #print(f'writing to file, index start {start_index} end {end_index} text {new_text}')\n",
    "    with open(newdir+entry.name, 'w', encoding=\"utf8\") as f2:\n",
    "        f2.write(new_text)\n",
    "    # determine if there's page overlap between current and next chapter, \n",
    "    # in which case end title can be reused as start title\n",
    "    if next_chap_start_page == chapter_end_page:\n",
    "        end_title = last_page[end_index]\n",
    "    else: end_title = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501aa05-1799-4e0d-be8c-afe532894b9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "All in all, ~125,000 words were cast off in trimming--including, as I learned much later, one entire short chapter, which, all in all, is really not a bad result. \n",
    "\n",
    "**Given the labor involved, at this point I copied CC_ML_FR_trimmed_cleaned over into ARG isntead of actually replicating the process and turned the {n}'s back into newlines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "180f2b23-ffc5-4f83-89a2-7b4d1301898e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copytree('text-data/CC_ML_FR_trimmed_cleaned', 'text-data/ARG', dirs_exist_ok=True)\n",
    "for entry in os.scandir('text-data/ARG'):\n",
    "    with open(entry.path, 'r+', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('{n}', '\\n')\n",
    "        f.seek(0)\n",
    "        f.write(text)\n",
    "        f.truncate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9cb33-1fbb-47a7-bf34-e71b18b5398f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clearing Out Non-English Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88b6a8-00ae-40d0-9f3f-e46e5010b2fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "On many occasions, Hakluyt presents non-English text (usually Latin, with some Spanish and a few Portuguese, Italian and possibly French) followed by a translation. The vast majority of the text is English, so this has no effect on word frequency counts, but it comes up as distracting noise in topic modeling and network graphs, so I eventually decided to trim it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0f9ccefc-b24c-4478-8745-426d32691dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "identifier.set_languages(['en','la','es'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6e967b2e-5648-4db2-9fae-e1166395a7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source = 'text-data/ARG'\n",
    "target = 'text-data/ARG_EN/'\n",
    "os.mkdir(target)\n",
    "for entry in os.scandir(source):\n",
    "    with open(entry.path, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        #splitting into paragraphs since what I'm looking to weed out is large non-English sections\n",
    "        #(usually lasting half a document) rather than disparate words or sentences\n",
    "        parags = text.split('\\n')\n",
    "        EN_parags = []\n",
    "        for parag in parags:\n",
    "            if langid.classify(parag)[0] == 'en':\n",
    "                EN_parags.append(parag)\n",
    "        EN_text = '\\n'.join(EN_parags)\n",
    "        # control for small bits of English misidentified\n",
    "        if len(EN_text) > 0.7* len(text):\n",
    "            EN_text = text\n",
    "        with open(target + entry.name, 'w', encoding=\"utf8\") as fw:\n",
    "            fw.write(EN_text+'.') #adding a full stop to avoid zero-length files that will be a pain later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb5f0d-d40d-4ba4-b5b2-958cd27e17c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lemmatization & Spelling Normalization with MorphAdorner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed91a4-fc58-4a0f-8558-97146b4dd43e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Even a perfectly transcribed copy of the *Navigations* would still be posing a great deal of trouble for computational analysis due to the inconsistency of Early Modern English and its distance from current English standards. Further, for many operations, such as word frequency counts and topic modeling, it is desirable to bring words down to their basic disctionary forms, allowing, say, \"Indians\" to be counted as an instance of \"Indian\" rather than as a separate entity; the process is known as lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31547b-e5ec-4ecb-a973-c134e2d65342",
   "metadata": {
    "tags": []
   },
   "source": [
    "NLTK lemmatization does not handle Hakluyt's text particularly well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2797acc7-032f-4426-a661-af131768dea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kingdome : kingdome\n",
      "doe : doe\n",
      "us : u\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in ['kingdome', 'doe', 'us']:\n",
    "    print(w,\":\", lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc46aea-982a-4dba-ab1f-053ccb8586ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "I initially thought to use a spelling corrector, but didn't find one at the time. Coming back to the idea now, I find that it would not have gotten me far either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eb831f51-ec36-4e94-bf50-92d604e5ec4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are to understand, that at the feast of Master, there was a great companies of \n",
      "Nobles with Hope John and Conradus the Emperor assembled at Some\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "t= '''YOu are to understand, that at the feast of Easter, there was a great companie of \n",
    "Nobles with Pope John and Conradus the Emperour assembled at Rome'''\n",
    "textBlb = TextBlob(t) \n",
    "textCorrected = textBlb.correct()\n",
    "print(textCorrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1da7d3-e825-4b3e-ab69-a5cbd990070e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Fortunately, folks at Northwestern have developed [MorphAdorner](http://morphadorner.northwestern.edu/morphadorner/), a tool for spelling standardization, lemmatization, and a few other operations, tailored to multiple out-of-date English variants, including Early Modern. MorphAdorner runs from the command line, and (in the simple implementation I used) returns a text file where each original token gets a line with a tab-delimited list of the following: original token, part-of-speech tag, normalized spelling, lemma; it is easy to feed that back into Python and generate new text files with either standardized spelling or lemmatized tokens.\n",
    "\n",
    "MorphAdorner commands:\n",
    "\n",
    "- cd C:\\Users\\apovzner\\Documents\\morphadorner-2.0.1 (or wherever you put the working folder; see [installation instructions](http://morphadorner.northwestern.edu/morphadorner/download/))\n",
    "\n",
    "- adornplainemetext C:\\Users\\apovzner\\Documents\\Hakluyt\\text-data\\morphadorner-outputs\\ARG_EN C:\\Users\\apovzner\\Documents\\Hakluyt\\text-data\\ARG_EN\\\\*.txt\n",
    "  - adornplainemetext: for adorning Early Modern English plaintext files; see [here](http://morphadorner.northwestern.edu/morphadorner/documentation/adorningatext/) for more options\n",
    "  - full format: [command] \\outputdir \\inputdir\\ [specific file or wildcard format]\n",
    "  \n",
    "Since MorphAdorner produces files that require further processing, I've concentrated all those in the separate morphadorner-outputs folder. It does not preserve newlines, so I start by replacing them with a flag, \"{n}\", and then convert those back into newlines after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d305c7f0-f7e4-47ef-b76d-4f165f0308c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for entry in os.scandir('text-data/ARG_EN'):\n",
    "    with open(entry.path, 'r+', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('\\n', '{n}')\n",
    "        f.seek(0)\n",
    "        f.write(text)\n",
    "        f.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0554ce3b-c58d-46dc-a564-a691a6865e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adorned_extract(adorn_output, target, mode):\n",
    "    '''\n",
    "    takes MorphAdorner's output and converts it to a lemmatized/normalized-spelling plaintext,\n",
    "    substituting newlines for '{ n }'\n",
    "    adorn_output: folder path of MorphAdorner's output\n",
    "    target: new corpus folder\n",
    "    mode: 'lem' for lemmatization or 'spel' for normalized spelling\n",
    "    '''\n",
    "    if mode == 'lem':\n",
    "        col = 4\n",
    "    elif mode == 'spel':\n",
    "        col = 3\n",
    "    else:\n",
    "        raise ValueError('unrecognized mode requested')\n",
    "    os.mkdir(target)\n",
    "    filelist = os.scandir(adorn_output)\n",
    "    for entry in filelist:\n",
    "        if entry.is_file():\n",
    "            with open(entry.path, 'r', encoding=\"utf8\") as f:\n",
    "                new_text = ''\n",
    "                for line in f:\n",
    "                    #extracting token-by-token from either the lemmatized or spel-normalized column\n",
    "                    new_text += (line.split()[col] + ' ')\n",
    "                #recovering original line breaks\n",
    "                new_text = new_text.replace('{ n }', '\\n')\n",
    "                with open((target + '/' + entry.name), 'w', encoding=\"utf8\") as fw:\n",
    "                    fw.write(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2366ed4b-e7d2-4fe6-b60d-435376956ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adorned_extract('text-data/morphadorner-outputs/ARG_EN', 'text-data/ARG_EN_lem', 'lem')\n",
    "adorned_extract('text-data/morphadorner-outputs/ARG_EN', 'text-data/ARG_EN_spel', 'spel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9681f7a-2401-43af-b8e9-188f00da5b47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Basic Numbers Rundown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
