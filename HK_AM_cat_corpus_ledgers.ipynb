{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d608ecc-fc80-4655-b5fb-aab562568f07",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 0: initial ledger creation\n",
    "\n",
    "### work on hand-tagged American intercultural encounter segments\n",
    "\n",
    "#### 1: Encode Am tags from ledger into filenames; only include expl and/or enc tagged\n",
    "\n",
    "- name format:\n",
    "    - vol_chap-date-nat-enc-expl-title-pagerange\n",
    "    - enc_full enc_part enc_min enc_none\n",
    "    - expl_y expl_n\n",
    "\n",
    "underscores within features, dashes between features\n",
    "\n",
    "- for entry in ledger:\n",
    "    - if enc OR expl:\n",
    "        - locate file by vol-chap\n",
    "        - save file in new dir with new name\n",
    "\n",
    "#### 2: prepare ledger for re-integration with rest of corpus\n",
    "#### 3: update merged ledger with EN-only wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df28a401-2846-4aa0-8bc1-f01d51be0586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "punctuation = [i for i in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a240f1a4-2a3c-434b-9965-ed0c64e03195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Using the CambridgeCore MacLehose pdfminer extracted text files (with hand-corrected dates), creates a CSV file tracking the metadata from the filenames (with an eye to adding more fields in the future, e.g., for genre of voyage / letter / testimony / etc)\n",
    "\n",
    "# # set working directory\n",
    "# import os\n",
    "# os.chdir('text-data/CambridgeCore MacLehose pdfminer extract')\n",
    "# import csv\n",
    "# #01_06_NNE-_0624_The_voyage_of_Bertus_into_Ireland_Anno_684_pp.010-010.pdf filename format for reference\n",
    "# # open the file in the write mode\n",
    "# with open('ledger.csv', 'w', newline = '') as ledger:\n",
    "    \n",
    "#     # create the csv writer\n",
    "#     writer = csv.writer(ledger)\n",
    "    \n",
    "#     # create header\n",
    "#     writer.writerow(['vol', 'chap', 'geog', 'date', 'title', 'pages'])\n",
    "\n",
    "#     filelist = os.scandir(os.getcwd())\n",
    "#     for entry in filelist:\n",
    "#         row = [entry.name[:2], entry.name[3:5], entry.name[6:10], entry.name[11:15], entry.name[16:-15], entry.name[-11:-4]]\n",
    "#         #print(entry.name, '\\n', row)\n",
    "#         # write a row to the csv file\n",
    "#         writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7497e62-2cac-4445-928f-6f4c2722b131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # code below still misses a couple of files--but better than the initial version, which I apparently wrote to just catch enc/expl entries, and totally forgot about that filter\n",
    "# old_path = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat'\n",
    "# new_path = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat_mod'\n",
    "\n",
    "# import csv\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "\n",
    "# with open('text-data/ledger_merged_2.csv', mode = 'r') as ledger_csv:\n",
    "#     ledger = csv.DictReader(ledger_csv)\n",
    "#     for row in ledger:\n",
    "#         if row['geog'] == 'AMER':\n",
    "#             filelist = os.scandir(old_path)\n",
    "#             for entry in filelist:\n",
    "#                 #print('scanning' , entry.name)\n",
    "#                 #print ( 'condition checks: ', row['vol'].zfill(2), re.search(r'_(.*?)_', entry.name).group(1), row['vol'].zfill(2) == entry.name[:2], row['chap'] == re.search(r'_(.*?)_', entry.name).group(1))\n",
    "#                 if row['vol'].zfill(2) == entry.name[:2] and (row['chap'].zfill(2) == re.search(r'_(.*?)_', entry.name).group(1) or row['chap'].zfill(2) == re.search(r'_(.*?)_', entry.name).group(1)):\n",
    "#                     #print(entry.name, 'passed')\n",
    "#                     # find the relevant text file\n",
    "#                     with open(entry.path, 'r', encoding=\"utf8\") as f:\n",
    "#                         text = f.read()\n",
    "#                         expl = 'expl_y' if row['expl'] == 'v' else 'expl_n'\n",
    "#                         if row['enc'] == 'v' and row['expl'] != 'v':\n",
    "#                             enc = 'enc_full'\n",
    "#                         elif row['enc'] == 'v':\n",
    "#                             enc = 'enc_part'\n",
    "#                         elif row['enc_min'] == 'v':\n",
    "#                             enc = 'enc_min'\n",
    "#                         else:\n",
    "#                             enc = 'enc_none'\n",
    "#                         # determine name factors\n",
    "#                         name = row['vol'].zfill(2) + '_' + row['chap'].zfill(2) + '-' + row['date'] + '-' + row['nat'] + '-' + enc + '-' + expl + '-' + row['title'] + '-' + row['pages'].replace('-', '_') + '.txt'\n",
    "#                         # string formatted filename\n",
    "#                         with open((new_path + '/' + name), 'w', encoding=\"utf8\") as fw:\n",
    "#                             fw.write(text)\n",
    "#                             #print('text outputted')\n",
    "#                         #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "089c6e7a-985a-45cd-8867-d55b86ac8c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vol(name):\n",
    "    return(name[:2])\n",
    "def get_chap(name):\n",
    "    name = name[3:]\n",
    "    return(name[:name.find('_')])\n",
    "def wordcount(text):\n",
    "    '''splits texts on whitespace and counts the segments, ignoring punctuation'''\n",
    "    tokens = text.split()\n",
    "    words = [w for w in tokens if w not in punctuation]\n",
    "    return(len(words))\n",
    "def wordcount_f(vol, chap, path):\n",
    "    '''locates file according to vol/chap in path, splits texts on whitespace and counts the segments, ignoring punctuation'''\n",
    "    filelist = os.scandir(path)\n",
    "    for entry in filelist:\n",
    "        if int(entry.name[:2]) == int(vol):\n",
    "            #for full chapter files:\n",
    "            if '.' not in chap:\n",
    "                if int(entry.name[3:5]) == int(chap):\n",
    "                    with open(entry.path, mode = 'r', encoding=\"utf8\") as f:\n",
    "                        text = f.read()\n",
    "                        return(wordcount(text))\n",
    "            else:\n",
    "                #for American sectioned files:\n",
    "                if entry.name[entry.name.find('_')+1:entry.name.find('AMER')-1] == chap:\n",
    "                    with open(entry.path, mode = 'r', encoding=\"utf8\") as f:\n",
    "                        text = f.read()\n",
    "                        return(wordcount(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76e73cc2-3574-481d-ae64-2f6126b83d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# testseg = '07_17.2_AMER_1170_The_most_ancient_voyage_and_discovery_of_the_West_Indies_performed_by_Madoc_the_sonne_of_Owen_Guined_pp.133-135.txt'\n",
    "# testfull = '09_24_AMER_1572_The_memorable_voyage_of_M_John_Chilton_to_all_the_principall_parts_of_Nueva_Espanna_pp.360-377'\n",
    "# #print(get_chap(test))\n",
    "# print(testfull[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8d09544-a538-4fbb-96b3-c0f0c5a66d71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# chap = '22.4'\n",
    "# print(chap[:chap.find('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "303da1f9-194b-4541-aca4-33b7b173823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24102\n"
     ]
    }
   ],
   "source": [
    "# #spot-wordcounts for missing lines\n",
    "# missing_files = ['11_29_AMER_1579_A_discourse_of_the_West_Indies_and_the_South_sea_written_by_Lopez_Vaz_a_Portugall_conteining_divers__pp.227-290.txt']\n",
    "# missing_paths = ['text-data/CC_ML_FR_trimmed_morphad_lem_AMER/' + f for f in missing_files]\n",
    "# for path in missing_paths:\n",
    "#     with open(path, mode = 'r', encoding=\"utf8\") as f:\n",
    "#         text = f.read()\n",
    "#         print(wordcount(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58251fe9-e515-48e5-96f7-3578fe79d55d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2: create an integrated full + AM_cat ledger\n",
    "\n",
    "1. compute & record segment wordcount\n",
    "1. [manually merge enc ledger with full ledger]\n",
    "1. compute & record full chapter wordcounts\n",
    "1. compute section fractions\n",
    "1. update the following fields from sections into parent chapters: national_perspective, exploration_component, intercultural_encounter_component, minor_intercultural_encounter_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9486af4-233b-4658-b64f-dbd1c416124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #now obsolete code merging AMledger with full ledger\n",
    "\n",
    "# cat_path = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat_EN/'\n",
    "# lem_path = 'CC_ML_FR_trimmed_cleaned_EN_cl_MAlem'\n",
    "\n",
    "# with open('text-data/ledger_merged_4.csv', mode = 'r') as ledger_csv:\n",
    "#     full_ledger = csv.DictReader(ledger_csv) \n",
    "#     with open('text-data/AMledger_wordcounts.csv', mode = 'r') as AMledger_csv:\n",
    "#         AM_ledger = csv.DictReader(AMledger_csv)\n",
    "#         #writing header row\n",
    "#         with open('text-data/ledger_merged_EN.csv', mode = 'a', newline='') as ledger_new_csv:\n",
    "#             writer = csv.DictWriter(ledger_new_csv, ledger.fieldnames)\n",
    "#             writer.writer.writerow(ledger.fieldnames)\n",
    "#             #copy over all non-Am rows from full ledger + wordcounts + sect_f = 1 (includes header #2)\n",
    "#             for row in full_ledger:\n",
    "#                 if row['geog'] != 'AMER':\n",
    "#                     #except header # 2 from wordcount/sect\n",
    "#                     if row['vol'] != 'volume':\n",
    "#                         row['word'] = wordcount_f(row['vol'], row['chap'])\n",
    "#                         row['sect_frac'] = 1\n",
    "#                     writer.writerow(row)\n",
    "#             #for Am rows:\n",
    "#             #for full chapters in am ledger, just copy am rows into the new ledgers  + sect_f = 1\n",
    "#             #when catches section: find corresponding full chapter;  + wordcount + sect_f = 1\n",
    "#             # then write in all the sectioned rows while calculating fractions\n",
    "#             #starting scan on am_ledger\n",
    "#             for row in AM_ledger:\n",
    "#                 #for non-header rows:\n",
    "#                 if row['geog'] == 'AMER':\n",
    "#                     if '.' not in row['chap']:\n",
    "#                         #for full chapters in am ledger, just copy am rows into the new ledgers +  sect_f = 1\n",
    "#                         filelist = os.scandir(parent_path)\n",
    "#                         row['sect_frac'] = 1\n",
    "#                         writer.writerow(row)\n",
    "#                     else:\n",
    "#                         #sequence of sections begins:  find corresponding full chapter row, write in + wordcount + sect_f = 1\n",
    "#                                                     # then write in all the sectioned rows while calculating fractions\n",
    "#                         #print(row)\n",
    "#                         if row['chap'][-1] == '1':\n",
    "#                             #bring over row from ful ledger\n",
    "#                             vol = row['vol']\n",
    "#                             chap = row['chap'][:row['chap'].find('.')]\n",
    "#                             print(f'looking for {vol}, {chap}')\n",
    "#                             with open('text-data/ledger_merged_1.csv', mode = 'r') as ledger3_csv:\n",
    "#                                 full_ledger2 = csv.DictReader(ledger3_csv) \n",
    "#                                 for row_f in full_ledger2:\n",
    "#                                     #print(row_f)\n",
    "#                                     if row_f['vol'] != 'volume':\n",
    "#                                         if int(row_f['vol']) == int(vol) and int(row_f['chap']) == int(chap):\n",
    "#                                             wordcount_full = wordcount_f(vol, chap)\n",
    "#                                             row_f['word'] = wordcount_full\n",
    "#                                             row_f['sect_frac'] = 1\n",
    "#                                             writer.writerow(row_f)\n",
    "#                                             break\n",
    "#                         row['sect_frac'] = int(row['word']) / wordcount_full\n",
    "#                         writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19909fe9-2806-4457-a082-c9877a3c262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #checking Eric's suggestion of 17 mil words\n",
    "# t_count = w_count = 0\n",
    "# filelist = os.scandir('text-data/CC_ML_FR_trimmed_cleaned_MAspel')\n",
    "# for entry in filelist:\n",
    "#     with open(entry.path, mode = 'r', encoding=\"utf8\") as f:\n",
    "#         text = f.read()\n",
    "#         tokens = text.split()\n",
    "#         words = [w for w in tokens if w not in punctuation]\n",
    "#         t_count += len(tokens)\n",
    "#         w_count += len(words)\n",
    "# print(f'{t_count} tokens, {w_count} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1ee1e03-6b2b-4082-8c9f-fc9adf87941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #update merged ledger with EN-only wordcounts\n",
    "# cat_path = 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat_EN/'\n",
    "# lem_path = 'CC_ML_FR_trimmed_cleaned_EN_cl_MAlem'\n",
    "\n",
    "# with open('text-data/ledger_merged_3.csv', mode = 'r') as ledger_csv:\n",
    "#     full_ledger = csv.DictReader(ledger_csv) \n",
    "#     with open('text-data/ledger_merged_EN.csv', mode = 'a', newline='') as ledger_new_csv:\n",
    "#         writer = csv.DictWriter(ledger_new_csv, full_ledger.fieldnames)\n",
    "#         #writing header row\n",
    "#         writer.writeheader()\n",
    "#         #writer.writer.writerow(ledger.fieldnames)\n",
    "#         #simply update non-Am rows from full ledger with new wordcounts\n",
    "#         for row in full_ledger:\n",
    "#             #add wordcount to row\n",
    "#             if row['geog'] != 'AMER':\n",
    "#                 row['word'] = wordcount_f(row['vol'], row['chap'], 'text-data/CC_ML_FR_trimmed_cleaned_EN_cl_MAlem')\n",
    "#                 #print('non-am row')\n",
    "#             else:\n",
    "#                 #for full chapters, wordcount as usual and save into a temp variable for calculating fractions \n",
    "#                 if '.' not in row['chap']:\n",
    "#                     row['word'] = wordcount_f(row['vol'], row['chap'], 'text-data/CC_ML_FR_trimmed_cleaned_EN_cl_MAlem')\n",
    "#                     parent_wordcount = row['word'] + 0.0001 #avoid div-zero\n",
    "#                     #print('am-row non-sectioned')\n",
    "#             #for Am rows:\n",
    "#             #check for sectioned text file in AMcat folder \n",
    "#             #when catches section: find corresponding full chapter;  + wordcount + sect_f = 1\n",
    "#             # then write in all the sectioned rows while calculating fractions\n",
    "#                 else:\n",
    "#                     print('am sect check started')\n",
    "#                     AM_filelist = os.scandir(cat_path)\n",
    "#                     for entry in AM_filelist:\n",
    "#                         # complicated slicing for chapter value as get_chap above is not tuned to segmented sections\n",
    "#                         if int(get_vol(entry.name)) == int(row['vol']) and entry.name[entry.name.find('_')+1:entry.name.find('AMER')-1] == row['chap']:\n",
    "#                                 print(row, 'am row sectioned chap check passed')\n",
    "#                                 row['word'] = wordcount_f(row['vol'], row['chap'], 'text-data/CC_ML_FR_trimmed_MAlem_AMER_cat_EN')\n",
    "#                                 row['sect_frac'] = row['word'] / parent_wordcount\n",
    "#             #write row as composed in either decision tree branch\n",
    "#             writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f012c090-6c9b-4316-8e6b-4585b14705a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.6\n"
     ]
    }
   ],
   "source": [
    "# entryname = '07_23.6_AMER_1497_A_note_of_Cabots_first_discoverie_from_Robert_Fabians_Chronicle_pp.154-154'\n",
    "# print(entryname[entryname.find('_')+1:entryname.find('AMER')-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d970144-a396-4c98-bc7c-ba7968d2ee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-32.3\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
